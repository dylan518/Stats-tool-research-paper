{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import least_squares\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sympy\n",
    "from scipy.optimize import minimize\n",
    "import numpy\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.datasets import make_regression\n",
    "# Import function to create training and test set splits\n",
    "# Import function to automatically create polynomial features! \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Import Linear Regression and a regularized regression function\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "# Finally, import function to make a machine learning pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pylab\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test: 2000 y_test: x: 2000 y: 2000\n",
      "intial_est: [1.48808594]\n",
      "predicted randomnessw: 0.13609811057384386\n"
     ]
    }
   ],
   "source": [
    "#variance_depth=\n",
    "r_asumb=0\n",
    "q=[]\n",
    "lrn_r_depth=30\n",
    "#nmbr_pnts=\n",
    "#rndmnss=\n",
    "nmbr_data_points=2000\n",
    "lrning_depth=5\n",
    "h_lim=0.01\n",
    "intial_est=20\n",
    "data_randomness=5\n",
    "min_step=-2\n",
    "lrn_r_depth=10\n",
    "predicted_randomness=9\n",
    "euler_est=2\n",
    "r_asum=0\n",
    "qsum=0\n",
    "pylab.ylim([0,2])\n",
    "pylab.xlim([0,2])\n",
    "df_crp=pd.read_csv(\"C:/Users/dylan/Downloads/Production_Crops_E_Africa.csv\",encoding = \"ISO-8859-1\")\n",
    "#df_crp=df_crp.drop(['Item Code'], axis=1)\n",
    "df_crp=df_crp.drop(['Unit'], axis=1)\n",
    "df_crp=df_crp.drop(['Element Code'], axis=1)\n",
    "df_crp=df_crp.drop(['Area'], axis=1)\n",
    "df_crp=df_crp.drop(['Area Code'], axis=1)\n",
    "df_crp=df_crp.drop(['Item Code'], axis=1)\n",
    "dr_crp=df_crp.transpose()\n",
    "df_crp=df_crp[df_crp['Element'].str.contains(\"Yield\")]\n",
    "#df_crp=df_crp[df_crp['Element'].str.contains(\"Production\")]\n",
    "df_crp=df_crp[df_crp['Item'].str.contains(\"Cereals,Total\")]\n",
    "df_crp=df_crp.drop(['Element'], axis=1)\n",
    "df_crp=df_crp.drop(['Item'], axis=1)\n",
    "for i in range (0,54):\n",
    "    df_crp = df_crp.drop([df_crp.columns[i+1]],  axis='columns')\n",
    "\n",
    "df_crp = pd.DataFrame(np.repeat(df_crp.values,2,axis=0))\n",
    "df_crp.columns = df_crp.columns\n",
    "df_crp = df_crp.reindex(df_crp.columns.tolist() + ['last'], axis=1)  # version > 0.20.0\n",
    "df_crp=df_crp.transpose()\n",
    "\n",
    "\n",
    "#df_crp=df_crp.drop(['Element'], axis=1)\n",
    "#cols = list(df_crp.columns)\n",
    "#cols = [cols[-1]] + cols[:-1]\n",
    "#df_crp= df_crp[cols]\n",
    "dfx_crp=pd.DataFrame()\n",
    "dfy_crp=pd.DataFrame()\n",
    "X=[]\n",
    "y=[]\n",
    "for i in range(0,56):\n",
    "    df_crp[i*2]=df_crp[i*2].shift(periods=1)\n",
    "for i in range(0,56):\n",
    "    dfx_crp[i]=df_crp[i*2]\n",
    "    dfy_crp[i]=df_crp[i*2+1]\n",
    "for k in range(0,56):\n",
    "    X.append(dfx_crp[k])\n",
    "combinedx = pd.concat(X, ignore_index=True)\n",
    "y_testB=[]\n",
    "inputs_test=[]\n",
    "for k in range(0,56):\n",
    "    y.append(dfy_crp[k])\n",
    "combinedy = pd.concat(y, ignore_index=True)\n",
    "#dfx_crp=dfx_crp.stack(dropna=False)\n",
    "#dfy_crp=dfy_crp.stack(dropna=False)\n",
    "df_concat = pd.concat([combinedy, combinedx], axis=1)\n",
    "df_concat=df_concat.dropna(inplace=False)\n",
    "\n",
    "ytst=df_concat[df_concat.columns[1]].tail(2000)\n",
    "inputsi=df_concat[df_concat.columns[0]].tail(2000)\n",
    "\n",
    "\n",
    "\n",
    "inputs_test=ytst.tail(2000)\n",
    "y_testB=inputsi.tail(2000)\n",
    "inputs=df_concat[df_concat.columns[0]].head(2000)\n",
    "y=df_concat[df_concat.columns[1]].head(2000)\n",
    "\n",
    "y_testB=y_testB.values\n",
    "y=y.values\n",
    "inputs=inputs.values\n",
    "inputs_test=inputs_test.values\n",
    "print(\"x_test:\",len(inputs_test),\"y_test:\",\"x:\",len(inputs),\"y:\", len(y))\n",
    "yd=[]\n",
    "for i in range(0,len(y)):\n",
    "    yd.append(y[i]/inputs[i])\n",
    "xd=[]\n",
    "x_AV=(sum(inputs)/len(inputs))\n",
    "for i in range(0,len(y)):\n",
    "    xd.append(inputs[i]/x_AV)\n",
    "inputs=xd\n",
    "y=yd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ydt=[]\n",
    "for i in range(0,len(y_testB)):\n",
    "    ydt.append(y_testB[i]/inputs_test[i])\n",
    "xdt=[]\n",
    "x_AVt=(sum(inputs_test)/len(inputs_test))\n",
    "for i in range(0,len(y_testB)):\n",
    "    xdt.append(inputs_test[i]/x_AVt)\n",
    "inputs_test=xdt\n",
    "y_testB=ydt\n",
    "\n",
    "\n",
    "\n",
    "#inputs=np.random.randint(-1000, 1000, size=nmbr_data_points)/100\n",
    "#y=inputs**2\n",
    "b1=y\n",
    "#plt.scatter(inputs, y)\n",
    "#a=data_randomness\n",
    "#r1=(1+a)*1000\n",
    "#r2=(1-a)*1000\n",
    "#r=np.random.randint(r2, r1, size=nmbr_data_points)/1000\n",
    "#y=y*r\n",
    "#b1=y\n",
    "x_values=np.random.randint(0.03, 8, size=nmbr_data_points)/10\n",
    "t_min=[]\n",
    "e=10\n",
    "t=[]\n",
    "y_avg=sum(y)/len(y)\n",
    "y_var=(sum(abs(y_avg-y)))/len(y)\n",
    "yovar=y_var\n",
    "for q in range(0,lrn_r_depth): \n",
    "    \n",
    "\n",
    "    y_var=(yovar/(predicted_randomness))*0.3\n",
    "#    print(\"y_var:\",y_var)\n",
    "\n",
    "    def rosen(x):\n",
    "        if x<1:\n",
    "            x=1\n",
    "\n",
    "        t=[]\n",
    "        t_min=[]\n",
    "        for i in inputs:\n",
    "            y_xa=sum(y*(1/(x**(abs(i-inputs))))/sum(1/(x**(abs(i-inputs)))))\n",
    "            t.append(y_xa)\n",
    "        for i in x_values:\n",
    "            y_xb=sum(y*(1/(x**(abs(i-inputs))))/sum(1/(x**(abs(i-inputs)))))\n",
    "            t_min.append(y_xb)\n",
    "\n",
    "        t_avg=sum(t)/len(t)\n",
    "        t_var=(sum(abs(t_avg-t)))/len(t)\n",
    "        return(abs(t_var-y_var))\n",
    "        print(t_var-y_var)\n",
    "#        return(abs(t_var-y_var),((((sum(abs(t_min-y)))/len(t_min))),t_var,t_var-y_var))\n",
    "    #\n",
    "\n",
    "    \n",
    "    ####minimizer\n",
    "    x0 = np.array([2.0])\n",
    "    #    \n",
    "    #res = minimize(rosen, x0, method='nelder-mead',\n",
    "    #            options={\"maxiter\":500, \"maxfev\":500})\n",
    "    res = minimize(rosen, x0, method='nelder-mead',\n",
    "            options={\"maxiter\":500, \"maxfev\":500})\n",
    "    intial_est=res.x\n",
    "#    for i in range(0,lrning_depth):\n",
    "#        der1=((rosen(intial_est+h_lim)))[0]\n",
    "#        der2=(rosen(intial_est))[0]\n",
    "#        intial_est=intial_est+(((((rosen(intial_est+h_lim)[0]-rosen(intial_est)[0])))/h_lim))*((((min_step))))\n",
    "    if intial_est<-2:\n",
    "        intial_est=(np.random.randint(-200, 200)/10000)\n",
    "        print(\"no\")\n",
    "    print(\"intial_est:\",intial_est)\n",
    "    \n",
    "    #    print(rosen(intial_est),intial_est)\n",
    "\n",
    "    #    if intial_est<1:\n",
    "    #        intial_est=1.1\n",
    "    #        print(\"no\")\n",
    "\n",
    "    #    print(rosen(intial_est),y_var)\n",
    "    #    print(intial_est)\n",
    "    e=abs(intial_est)\n",
    "    r_est=[]\n",
    "    for i in inputs:\n",
    "        y_e=sum(y*(1/(e**(abs(i-inputs))))/sum(1/(e**(abs(i-inputs)))))\n",
    "    y_avgb=(sum(numpy.absolute(y)))/len(y)\n",
    "    r_a=(sum(abs(y/y_e-1)))/len(y)\n",
    "    r_est=(y_avgb/r_a)\n",
    "    \n",
    "\n",
    "    \n",
    "    e=abs(intial_est)\n",
    "\n",
    "\n",
    "    r_asumb=r_asumb+r_a\n",
    "    #rosen1=var_rose\n",
    "    \n",
    "    \n",
    "    r_asum=r_a+r_asum\n",
    "    qsum=1+qsum\n",
    "    r_a=(r_asum/qsum)*0.6+r_a*0.4\n",
    "    predicted_randomness=r_a+1\n",
    "    print(\"predicted randomnessw:\",r_a)\n",
    "    \n",
    "randomness_estimate=r_asum/qsum\n",
    "tg=[]\n",
    "print(randomness_estimate,\"actual\")\n",
    "x2_values=np.linspace(start = 0.03, stop = 2.5, num =10000)\n",
    "for i in x2_values:\n",
    "    y_x=sum(y*(1/(e**(abs(i-inputs))))/sum(1/(e**(abs(i-inputs)))))\n",
    "    tg.append(y_x)\n",
    "\n",
    "\n",
    "plt.scatter(inputs_test,y_testB)\n",
    "plt.scatter(x2_values,tg)\n",
    "plt.show()\n",
    "ar=rosen(e)\n",
    "print(ar)\n",
    "#print(r_est)\n",
    "target_column =pd.DataFrame(y)\n",
    "input2=inputs\n",
    "predictors = list(set(list(inputs))-set(target_column))\n",
    "y=numpy.array(y).reshape(-1,1)\n",
    "X =numpy.array(inputs).reshape(-1,1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "\n",
    "# Alpha (regularization strength) of LASSO regression\n",
    "lasso_eps = 0.0001\n",
    "lasso_nalpha=20\n",
    "lasso_iter=5000\n",
    "# Min and max degree of polynomials features to consider\n",
    "degree_min =2\n",
    "degree_max =8\n",
    "for degree in range(degree_min,degree_max+1):\n",
    "    model = make_pipeline(PolynomialFeatures(degree, interaction_only=False), LassoCV(eps=lasso_eps,n_alphas=lasso_nalpha,max_iter=lasso_iter,\n",
    "normalize=True,cv=5))\n",
    "    model.fit(X_train,y_train)\n",
    "    test_pred = np.array(model.predict(X_test))\n",
    "    RMSE=np.sqrt(np.sum(np.square(test_pred-y_test)))\n",
    "    test_score = model.score(X_test,y_test)\n",
    "print(list(model.predict([[i]])))\n",
    "z=[]\n",
    "for i in x2_values:\n",
    "    z.append(list(model.predict([[i]])))\n",
    "\n",
    "\n",
    "#plt.plot(x2_values,(x2_values)**2)\n",
    "print(input)\n",
    "pylab.ylim([0,2])\n",
    "pylab.xlim([0,2])\n",
    "plt.scatter(inputs_test,y_testB)\n",
    "plt.scatter(x2_values,z)\n",
    "plt.show()\n",
    "lasso_pred=[]\n",
    "Xaxis_pred=[]\n",
    "e=abs(intial_est)\n",
    "\n",
    "#y_testB=y_testB.tolist()\n",
    "#inputs_test=inputs_test.tolist()\n",
    "for i in range(0,len(y_testB)):\n",
    "    lasso_pred.append(list(model.predict([[inputs_test[i]]])))\n",
    "    a=sum(b1*(1/(e**(abs(inputs_test[i]-input2)))))\n",
    "    b=sum(1/(e**(abs(inputs_test[i]-input2))))\n",
    "    Xaxis_pred.append(a/b)\n",
    "#print((((y*(1/(e**(abs(inputs_test[1]))))))))\n",
    "\n",
    "#print(b)\n",
    "print(a)\n",
    "\n",
    "resultX=0\n",
    "resultL=0\n",
    "print(Xaxis_pred[1])\n",
    "for i in range(0,len(y_testB)):\n",
    "    resultX=resultX+abs(Xaxis_pred[i]/y_testB[i]-1)\n",
    "    resultL=resultL+abs(lasso_pred[i]/y_testB[i]-1)\n",
    "\n",
    "L_accuracy=resultL/len(y_testB)\n",
    "X_accuracy=resultX/len(y_testB)\n",
    "print(\"Lasso Accuracy=\",L_accuracy)\n",
    "print(\"X-axis Accuracy=\",X_accuracy)\n",
    "print(\"X-axis Accuracy=\",X_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crp=pd.read_csv(\"C:/Users/dylan/Downloads/Production_Crops_E_Africa.csv\",encoding = \"ISO-8859-1\")\n",
    "#df_crp=df_crp.drop(['Item Code'], axis=1)\n",
    "df_crp=df_crp.drop(['Unit'], axis=1)\n",
    "df_crp=df_crp.drop(['Element Code'], axis=1)\n",
    "df_crp=df_crp.drop(['Area'], axis=1)\n",
    "df_crp=df_crp.drop(['Area Code'], axis=1)\n",
    "df_crp=df_crp.drop(['Item Code'], axis=1)\n",
    "dr_crp=df_crp.transpose()\n",
    "df_crp=df_crp[df_crp['Element'].str.contains(\"Yield\")]\n",
    "#df_crp=df_crp[df_crp['Element'].str.contains(\"Production\")]\n",
    "df_crp=df_crp[df_crp['Item'].str.contains(\"Cereals,Total\")]\n",
    "df_crp=df_crp.drop(['Element'], axis=1)\n",
    "df_crp=df_crp.drop(['Item'], axis=1)\n",
    "for i in range (0,54):\n",
    "    df_crp = df_crp.drop([df_crp.columns[i+1]],  axis='columns')\n",
    "\n",
    "df_crp = pd.DataFrame(np.repeat(df_crp.values,2,axis=0))\n",
    "df_crp.columns = df_crp.columns\n",
    "df_crp = df_crp.reindex(df_crp.columns.tolist() + ['last'], axis=1)  # version > 0.20.0\n",
    "df_crp=df_crp.transpose()\n",
    "\n",
    "\n",
    "#df_crp=df_crp.drop(['Element'], axis=1)\n",
    "#cols = list(df_crp.columns)\n",
    "#cols = [cols[-1]] + cols[:-1]\n",
    "#df_crp= df_crp[cols]\n",
    "dfx_crp=pd.DataFrame()\n",
    "dfy_crp=pd.DataFrame()\n",
    "X=[]\n",
    "y=[]\n",
    "for i in range(0,56):\n",
    "    df_crp[i*2]=df_crp[i*2].shift(periods=1)\n",
    "for i in range(0,56):\n",
    "    dfx_crp[i]=df_crp[i*2]\n",
    "    dfy_crp[i]=df_crp[i*2+1]\n",
    "for k in range(0,56):\n",
    "    X.append(dfx_crp[k])\n",
    "combinedx = pd.concat(X, ignore_index=True)\n",
    "\n",
    "for k in range(0,56):\n",
    "    y.append(dfy_crp[k])\n",
    "combinedy = pd.concat(y, ignore_index=True)\n",
    "#dfx_crp=dfx_crp.stack(dropna=False)\n",
    "#dfy_crp=dfy_crp.stack(dropna=False)\n",
    "df_concat = pd.concat([combinedy, combinedx], axis=1)\n",
    "df_concat=df_concat.dropna(inplace=False)\n",
    "inputs=df_concat[df_concat.columns[0]].head(700)\n",
    "y=df_concat[df_concat.columns[1]].head(700)\n",
    "y=y.values\n",
    "inputs=inputs.values\n",
    "yd=[]\n",
    "for i in range(0,len(y)):\n",
    "    yd.append(inputs[i]/y[i])\n",
    "xd=[]\n",
    "x_AV=(sum(inputs)/len(inputs))\n",
    "for i in range(0,len(y)):\n",
    "    xd.append(inputs[i]/x_AV)\n",
    "inputs=xd\n",
    "y=yd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y1961</th>\n",
       "      <th>Y1961F</th>\n",
       "      <th>Y1962</th>\n",
       "      <th>Y1962F</th>\n",
       "      <th>Y1963</th>\n",
       "      <th>Y1963F</th>\n",
       "      <th>Y1964</th>\n",
       "      <th>Y1964F</th>\n",
       "      <th>Y1965</th>\n",
       "      <th>Y1965F</th>\n",
       "      <th>...</th>\n",
       "      <th>Y2010</th>\n",
       "      <th>Y2010F</th>\n",
       "      <th>Y2011</th>\n",
       "      <th>Y2011F</th>\n",
       "      <th>Y2012</th>\n",
       "      <th>Y2012F</th>\n",
       "      <th>Y2013</th>\n",
       "      <th>Y2013F</th>\n",
       "      <th>Y2014</th>\n",
       "      <th>Y2014F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>3462.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8140.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8038.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5302.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6063.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>14011.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14424.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16776.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>18131.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13692.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>8280.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8303.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7984.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8758.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9320.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>6293.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6624.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5520.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8144.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8883.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>5461.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4899.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5035.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5293.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5515.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>12006.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15178.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13732.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13993.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14603.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>3418.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3396.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3828.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>2759.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3298.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>3717.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4523.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3672.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>2176.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3044.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>4085.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4779.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4731.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5225.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4857.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>10627.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9951.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12030.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11565.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12258.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>9462.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10106.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9196.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9919.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9836.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>12226.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11640.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11067.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11782.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13319.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>6500.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4857.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>1778.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>1823.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>8639.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7575.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7754.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7819.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7686.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>16425.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>17129.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15915.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16760.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16810.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>4430.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5409.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5439.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6616.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7902.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>14474.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15162.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16764.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15821.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14795.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>5922.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5441.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7586.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6354.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5466.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>7628.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6320.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9315.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8228.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8557.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>11078.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11188.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10837.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11624.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10949.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>13623.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13551.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13196.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13835.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13696.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>7143.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7113.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9167.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9326.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10204.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>7846.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7944.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8100.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8312.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8238.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2166</th>\n",
       "      <td>6242.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7857.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7519.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7749.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7855.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>22711.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>18842.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>20808.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>21694.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>21387.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2404</th>\n",
       "      <td>6992.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7008.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6920.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6921.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6460.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>7717.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7712.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7714.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7727.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7722.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18509.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>18829.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>19379.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2762</th>\n",
       "      <td>29057.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>32596.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>33128.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>33301.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>37756.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>65043.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>72464.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>72690.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>72853.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>72308.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2942</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5123.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5758.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5912.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6586.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5998.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18328.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>19616.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>20468.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>21931.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>23254.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>7146.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7023.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7040.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7235.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7323.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3588</th>\n",
       "      <td>15872.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15139.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15981.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15943.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16026.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>15980.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15996.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16046.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16051.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16049.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3692</th>\n",
       "      <td>10753.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10805.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10390.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10877.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10640.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>11294.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8685.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9103.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9598.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7467.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3892</th>\n",
       "      <td>8164.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8096.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7882.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7776.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8092.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>18143.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15942.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>17681.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16888.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>17034.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4051</th>\n",
       "      <td>13400.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13687.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12567.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13638.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13037.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>11849.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12266.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11527.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11903.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12463.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4186</th>\n",
       "      <td>8144.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7871.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7921.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6208.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7600.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>16515.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15379.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14669.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13336.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13217.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4510</th>\n",
       "      <td>12447.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12067.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12497.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11996.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11430.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>17101.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15146.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>17448.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16615.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16276.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4595</th>\n",
       "      <td>8362.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8147.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8133.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8096.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7562.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>9091.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6631.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>2384.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7644.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6371.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4699</th>\n",
       "      <td>5750.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5230.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5098.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5253.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>11786.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11947.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12981.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13092.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13216.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4887</th>\n",
       "      <td>2147.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>2210.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>2553.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>2350.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3419.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>6697.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6612.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6755.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6617.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6728.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5161</th>\n",
       "      <td>17848.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>17616.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>17582.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>18064.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>17449.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>32717.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>33414.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>33996.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>35052.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>36821.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <td>9845.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10270.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8767.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10768.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8565.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>19069.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>20943.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>20871.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>20688.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15906.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5546</th>\n",
       "      <td>7075.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7294.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6448.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7486.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8119.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>17161.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9962.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15274.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15665.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15507.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5662</th>\n",
       "      <td>3698.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3788.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3745.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3761.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3405.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>9560.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13754.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10353.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11980.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16790.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5816</th>\n",
       "      <td>15126.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>17807.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12895.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16444.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>42984.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>39018.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>33896.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>32191.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>37651.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>4076.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10689.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8913.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8641.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8813.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>15475.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16144.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10166.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>18283.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14543.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6354</th>\n",
       "      <td>8774.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8582.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9111.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9265.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9357.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>10284.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10411.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6298.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6702.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7026.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6484</th>\n",
       "      <td>4158.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4193.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4207.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3982.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3836.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>4636.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5346.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5495.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5938.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5888.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680</th>\n",
       "      <td>5052.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5478.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5601.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5993.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4675.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>4893.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>3775.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5142.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4074.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4475.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6885</th>\n",
       "      <td>7430.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7828.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7637.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7178.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6871.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>15280.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13345.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12349.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15918.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7082</th>\n",
       "      <td>39474.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>39474.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>39474.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>39474.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>39474.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>74946.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>78067.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>77697.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>77865.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>78058.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>8540.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13591.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14375.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13619.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14542.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>19191.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>20867.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>21442.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>21441.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>19807.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7360</th>\n",
       "      <td>14815.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14815.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14815.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14815.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14815.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>15819.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>21028.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>21038.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>21017.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>20995.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7542</th>\n",
       "      <td>5494.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5443.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5733.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5883.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5887.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>11961.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9663.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12289.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11230.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11102.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7751</th>\n",
       "      <td>9406.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12113.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12209.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13602.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13040.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>17459.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>17498.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>15384.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>17768.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>18416.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7873</th>\n",
       "      <td>4894.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4854.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4673.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4638.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4635.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>5752.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4572.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11900.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9640.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7607.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8149</th>\n",
       "      <td>10991.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11421.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11280.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9139.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9114.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>41491.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>40136.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>42393.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>40407.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>48935.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8339</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6924.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7656.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12537.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5373.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5928.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6827.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8759</th>\n",
       "      <td>9131.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8112.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8776.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7836.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6988.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>4517.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5638.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8901</th>\n",
       "      <td>4693.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4861.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5411.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4778.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4829.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>11993.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13355.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13289.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13603.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9355.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9083</th>\n",
       "      <td>4650.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4774.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5133.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5021.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>4525.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>11874.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>12263.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11124.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>10901.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11531.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9367</th>\n",
       "      <td>4992.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7295.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7496.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6416.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7388.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>16960.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>17110.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16743.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>11320.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>17564.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9543</th>\n",
       "      <td>9023.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9037.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9266.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8844.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9105.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>19784.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>20776.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>20286.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>19983.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>20193.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9812</th>\n",
       "      <td>8057.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8313.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9586.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8044.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8014.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>16479.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13904.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>13148.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>14180.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>16780.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9860</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5917.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5441.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5429.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5429.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>8222.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8014.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7069.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>7889.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8235.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>25335.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>27314.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>26893.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>25324.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>27554.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10256</th>\n",
       "      <td>9197.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9059.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8225.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>8205.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>9308.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>...</td>\n",
       "      <td>7346.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5932.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6889.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>6694.0</td>\n",
       "      <td>Fc</td>\n",
       "      <td>5408.0</td>\n",
       "      <td>Fc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Y1961 Y1961F    Y1962 Y1962F    Y1963 Y1963F    Y1964 Y1964F  \\\n",
       "212     3462.0     Fc   8140.0     Fc   8038.0     Fc   5302.0     Fc   \n",
       "369     8280.0     Fc   8303.0     Fc   7984.0     Fc   8758.0     Fc   \n",
       "559     5461.0     Fc   4899.0     Fc   5035.0     Fc   5293.0     Fc   \n",
       "657     3418.0     Fc   3396.0     Fc   3828.0     Fc   2759.0     Fc   \n",
       "820     4085.0     Fc   4779.0     Fc   4731.0     Fc   5225.0     Fc   \n",
       "964     9462.0     Fc  10106.0     Fc   9196.0     Fc   9919.0     Fc   \n",
       "1089    6500.0     Fc   6500.0     Fc   6500.0     Fc   6500.0     Fc   \n",
       "1336    8639.0     Fc   7575.0     Fc   7754.0     Fc   7819.0     Fc   \n",
       "1500    4430.0     Fc   5409.0     Fc   5439.0     Fc   6616.0     Fc   \n",
       "1636    5922.0     Fc   5441.0     Fc   7586.0     Fc   6354.0     Fc   \n",
       "1749   11078.0     Fc  11188.0     Fc  10837.0     Fc  11624.0     Fc   \n",
       "1933    7143.0     Fc   7113.0     Fc   9167.0     Fc   9326.0     Fc   \n",
       "2166    6242.0     Fc   7857.0     Fc   7519.0     Fc   7749.0     Fc   \n",
       "2404    6992.0     Fc   7008.0     Fc   6920.0     Fc   6921.0     Fc   \n",
       "2483       NaN    NaN      NaN    NaN      NaN    NaN      NaN    NaN   \n",
       "2762   29057.0     Fc  32596.0     Fc  33128.0     Fc  33301.0     Fc   \n",
       "2942       NaN    NaN      NaN    NaN      NaN    NaN      NaN    NaN   \n",
       "3223       NaN    NaN      NaN    NaN      NaN    NaN      NaN    NaN   \n",
       "3464    7146.0     Fc   7023.0     Fc   7040.0     Fc   7235.0     Fc   \n",
       "3588   15872.0     Fc  15139.0     Fc  15981.0     Fc  15943.0     Fc   \n",
       "3692   10753.0     Fc  10805.0     Fc  10390.0     Fc  10877.0     Fc   \n",
       "3892    8164.0     Fc   8096.0     Fc   7882.0     Fc   7776.0     Fc   \n",
       "4051   13400.0     Fc  13687.0     Fc  12567.0     Fc  13638.0     Fc   \n",
       "4186    8144.0     Fc   7871.0     Fc   7921.0     Fc   6208.0     Fc   \n",
       "4510   12447.0     Fc  12067.0     Fc  12497.0     Fc  11996.0     Fc   \n",
       "4595    8362.0     Fc   8147.0     Fc   8133.0     Fc   8096.0     Fc   \n",
       "4699    5750.0     Fc   5714.0     Fc   5230.0     Fc   5098.0     Fc   \n",
       "4887    2147.0     Fc   2210.0     Fc   2553.0     Fc   2350.0     Fc   \n",
       "5161   17848.0     Fc  17616.0     Fc  17582.0     Fc  18064.0     Fc   \n",
       "5353    9845.0     Fc  10270.0     Fc   8767.0     Fc  10768.0     Fc   \n",
       "5546    7075.0     Fc   7294.0     Fc   6448.0     Fc   7486.0     Fc   \n",
       "5662    3698.0     Fc   3788.0     Fc   3745.0     Fc   3761.0     Fc   \n",
       "5816   15126.0     Fc  17807.0     Fc  12895.0     Fc  10000.0     Fc   \n",
       "6171    4076.0     Fc  10689.0     Fc   8913.0     Fc   8641.0     Fc   \n",
       "6354    8774.0     Fc   8582.0     Fc   9111.0     Fc   9265.0     Fc   \n",
       "6484    4158.0     Fc   4193.0     Fc   4207.0     Fc   3982.0     Fc   \n",
       "6680    5052.0     Fc   5478.0     Fc   5601.0     Fc   5993.0     Fc   \n",
       "6885    7430.0     Fc   7828.0     Fc   7637.0     Fc   7178.0     Fc   \n",
       "7082   39474.0     Fc  39474.0     Fc  39474.0     Fc  39474.0     Fc   \n",
       "7270    8540.0     Fc  13591.0     Fc  14375.0     Fc  13619.0     Fc   \n",
       "7360   14815.0     Fc  14815.0     Fc  14815.0     Fc  14815.0     Fc   \n",
       "7542    5494.0     Fc   5443.0     Fc   5733.0     Fc   5883.0     Fc   \n",
       "7751    9406.0     Fc  12113.0     Fc  12209.0     Fc  13602.0     Fc   \n",
       "7873    4894.0     Fc   4854.0     Fc   4673.0     Fc   4638.0     Fc   \n",
       "8149   10991.0     Fc  11421.0     Fc  11280.0     Fc   9139.0     Fc   \n",
       "8339       NaN    NaN      NaN    NaN      NaN    NaN      NaN    NaN   \n",
       "8539       NaN    NaN      NaN    NaN      NaN    NaN      NaN    NaN   \n",
       "8759    9131.0     Fc   8112.0     Fc   8776.0     Fc   7836.0     Fc   \n",
       "8901    4693.0     Fc   4861.0     Fc   5411.0     Fc   4778.0     Fc   \n",
       "9083    4650.0     Fc   4774.0     Fc   5133.0     Fc   5021.0     Fc   \n",
       "9367    4992.0     Fc   7295.0     Fc   7496.0     Fc   6416.0     Fc   \n",
       "9543    9023.0     Fc   9037.0     Fc   9266.0     Fc   8844.0     Fc   \n",
       "9812    8057.0     Fc   8313.0     Fc   9586.0     Fc   8044.0     Fc   \n",
       "9860       NaN    NaN      NaN    NaN      NaN    NaN      NaN    NaN   \n",
       "9992    8222.0     Fc   8014.0     Fc   7069.0     Fc   7889.0     Fc   \n",
       "10256   9197.0     Fc   9059.0     Fc   8225.0     Fc   8205.0     Fc   \n",
       "\n",
       "         Y1965 Y1965F  ...    Y2010 Y2010F    Y2011 Y2011F    Y2012 Y2012F  \\\n",
       "212     6063.0     Fc  ...  14011.0     Fc  14424.0     Fc  16776.0     Fc   \n",
       "369     9320.0     Fc  ...   6293.0     Fc   6624.0     Fc   5520.0     Fc   \n",
       "559     5515.0     Fc  ...  12006.0     Fc  15178.0     Fc  13732.0     Fc   \n",
       "657     3298.0     Fc  ...   3717.0     Fc   4523.0     Fc   3672.0     Fc   \n",
       "820     4857.0     Fc  ...  10627.0     Fc   9951.0     Fc  12030.0     Fc   \n",
       "964     9836.0     Fc  ...  12226.0     Fc  11640.0     Fc  11067.0     Fc   \n",
       "1089    4857.0     Fc  ...   2200.0     Fc   1778.0     Fc   1962.0     Fc   \n",
       "1336    7686.0     Fc  ...  16425.0     Fc  17129.0     Fc  15915.0     Fc   \n",
       "1500    7902.0     Fc  ...  14474.0     Fc  15162.0     Fc  16764.0     Fc   \n",
       "1636    5466.0     Fc  ...   7628.0     Fc   6320.0     Fc   9315.0     Fc   \n",
       "1749   10949.0     Fc  ...  13623.0     Fc  13551.0     Fc  13196.0     Fc   \n",
       "1933   10204.0     Fc  ...   7846.0     Fc   7944.0     Fc   8100.0     Fc   \n",
       "2166    7855.0     Fc  ...  22711.0     Fc  18842.0     Fc  20808.0     Fc   \n",
       "2404    6460.0     Fc  ...   7717.0     Fc   7712.0     Fc   7714.0     Fc   \n",
       "2483       NaN    NaN  ...  18509.0     Fc  18829.0     Fc  20000.0     Fc   \n",
       "2762   37756.0     Fc  ...  65043.0     Fc  72464.0     Fc  72690.0     Fc   \n",
       "2942       NaN    NaN  ...   5123.0     Fc   5758.0     Fc   5912.0     Fc   \n",
       "3223       NaN    NaN  ...  18328.0     Fc  19616.0     Fc  20468.0     Fc   \n",
       "3464    7323.0     Fc  ...      NaN    NaN      NaN    NaN      NaN    NaN   \n",
       "3588   16026.0     Fc  ...  15980.0     Fc  15996.0     Fc  16046.0     Fc   \n",
       "3692   10640.0     Fc  ...  11294.0     Fc   8685.0     Fc   9103.0     Fc   \n",
       "3892    8092.0     Fc  ...  18143.0     Fc  15942.0     Fc  17681.0     Fc   \n",
       "4051   13037.0     Fc  ...  11849.0     Fc  12266.0     Fc  11527.0     Fc   \n",
       "4186    7600.0     Fc  ...  16515.0     Fc  15379.0     Fc  14669.0     Fc   \n",
       "4510   11430.0     Fc  ...  17101.0     Fc  15146.0     Fc  17448.0     Fc   \n",
       "4595    7562.0     Fc  ...   9091.0     Fc   6631.0     Fc   2384.0     Fc   \n",
       "4699    5253.0     Fc  ...  11786.0     Fc  11947.0     Fc  12981.0     Fc   \n",
       "4887    3419.0     Fc  ...   6697.0     Fc   6612.0     Fc   6755.0     Fc   \n",
       "5161   17449.0     Fc  ...  32717.0     Fc  33414.0     Fc  33996.0     Fc   \n",
       "5353    8565.0     Fc  ...  19069.0     Fc  20943.0     Fc  20871.0     Fc   \n",
       "5546    8119.0     Fc  ...  17161.0     Fc   9962.0     Fc  15274.0     Fc   \n",
       "5662    3405.0     Fc  ...   9560.0     Fc  13754.0     Fc  10353.0     Fc   \n",
       "5816   16444.0     Fc  ...  42984.0     Fc  39018.0     Fc  33896.0     Fc   \n",
       "6171    8813.0     Fc  ...  15475.0     Fc  16144.0     Fc  10166.0     Fc   \n",
       "6354    9357.0     Fc  ...  10284.0     Fc  10411.0     Fc   6298.0     Fc   \n",
       "6484    3836.0     Fc  ...   4636.0     Fc   5346.0     Fc   5495.0     Fc   \n",
       "6680    4675.0     Fc  ...   4893.0     Fc   3775.0     Fc   5142.0     Fc   \n",
       "6885    6871.0     Fc  ...  15280.0     Fc  13345.0     Fc  14000.0     Fc   \n",
       "7082   39474.0     Fc  ...  74946.0     Fc  78067.0     Fc  77697.0     Fc   \n",
       "7270   14542.0     Fc  ...  19191.0     Fc  20867.0     Fc  21442.0     Fc   \n",
       "7360   14815.0     Fc  ...  15819.0     Fc  21028.0     Fc  21038.0     Fc   \n",
       "7542    5887.0     Fc  ...  11961.0     Fc   9663.0     Fc  12289.0     Fc   \n",
       "7751   13040.0     Fc  ...  17459.0     Fc  17498.0     Fc  15384.0     Fc   \n",
       "7873    4635.0     Fc  ...   5752.0     Fc   4572.0     Fc  11900.0     Fc   \n",
       "8149    9114.0     Fc  ...  41491.0     Fc  40136.0     Fc  42393.0     Fc   \n",
       "8339       NaN    NaN  ...      NaN    NaN      NaN    NaN   6924.0     Fc   \n",
       "8539       NaN    NaN  ...      NaN    NaN      NaN    NaN   5373.0     Fc   \n",
       "8759    6988.0     Fc  ...   4517.0     Fc   5638.0     Fc      NaN    NaN   \n",
       "8901    4829.0     Fc  ...  11993.0     Fc  13355.0     Fc  13289.0     Fc   \n",
       "9083    4525.0     Fc  ...  11874.0     Fc  12263.0     Fc  11124.0     Fc   \n",
       "9367    7388.0     Fc  ...  16960.0     Fc  17110.0     Fc  16743.0     Fc   \n",
       "9543    9105.0     Fc  ...  19784.0     Fc  20776.0     Fc  20286.0     Fc   \n",
       "9812    8014.0     Fc  ...  16479.0     Fc  13904.0     Fc  13148.0     Fc   \n",
       "9860       NaN    NaN  ...   5714.0     Fc   5917.0     Fc   5441.0     Fc   \n",
       "9992    8235.0     Fc  ...  25335.0     Fc  27314.0     Fc  26893.0     Fc   \n",
       "10256   9308.0     Fc  ...   7346.0     Fc   5932.0     Fc   6889.0     Fc   \n",
       "\n",
       "         Y2013 Y2013F    Y2014 Y2014F  \n",
       "212    18131.0     Fc  13692.0     Fc  \n",
       "369     8144.0     Fc   8883.0     Fc  \n",
       "559    13993.0     Fc  14603.0     Fc  \n",
       "657     2176.0     Fc   3044.0     Fc  \n",
       "820    11565.0     Fc  12258.0     Fc  \n",
       "964    11782.0     Fc  13319.0     Fc  \n",
       "1089    1823.0     Fc   2018.0     Fc  \n",
       "1336   16760.0     Fc  16810.0     Fc  \n",
       "1500   15821.0     Fc  14795.0     Fc  \n",
       "1636    8228.0     Fc   8557.0     Fc  \n",
       "1749   13835.0     Fc  13696.0     Fc  \n",
       "1933    8312.0     Fc   8238.0     Fc  \n",
       "2166   21694.0     Fc  21387.0     Fc  \n",
       "2404    7727.0     Fc   7722.0     Fc  \n",
       "2483   20000.0     Fc  19379.0     Fc  \n",
       "2762   72853.0     Fc  72308.0     Fc  \n",
       "2942    6586.0     Fc   5998.0     Fc  \n",
       "3223   21931.0     Fc  23254.0     Fc  \n",
       "3464       NaN    NaN      NaN    NaN  \n",
       "3588   16051.0     Fc  16049.0     Fc  \n",
       "3692    9598.0     Fc   7467.0     Fc  \n",
       "3892   16888.0     Fc  17034.0     Fc  \n",
       "4051   11903.0     Fc  12463.0     Fc  \n",
       "4186   13336.0     Fc  13217.0     Fc  \n",
       "4510   16615.0     Fc  16276.0     Fc  \n",
       "4595    7644.0     Fc   6371.0     Fc  \n",
       "4699   13092.0     Fc  13216.0     Fc  \n",
       "4887    6617.0     Fc   6728.0     Fc  \n",
       "5161   35052.0     Fc  36821.0     Fc  \n",
       "5353   20688.0     Fc  15906.0     Fc  \n",
       "5546   15665.0     Fc  15507.0     Fc  \n",
       "5662   11980.0     Fc  16790.0     Fc  \n",
       "5816   32191.0     Fc  37651.0     Fc  \n",
       "6171   18283.0     Fc  14543.0     Fc  \n",
       "6354    6702.0     Fc   7026.0     Fc  \n",
       "6484    5938.0     Fc   5888.0     Fc  \n",
       "6680    4074.0     Fc   4475.0     Fc  \n",
       "6885   12349.0     Fc  15918.0     Fc  \n",
       "7082   77865.0     Fc  78058.0     Fc  \n",
       "7270   21441.0     Fc  19807.0     Fc  \n",
       "7360   21017.0     Fc  20995.0     Fc  \n",
       "7542   11230.0     Fc  11102.0     Fc  \n",
       "7751   17768.0     Fc  18416.0     Fc  \n",
       "7873    9640.0     Fc   7607.0     Fc  \n",
       "8149   40407.0     Fc  48935.0     Fc  \n",
       "8339    7656.0     Fc  12537.0     Fc  \n",
       "8539    5928.0     Fc   6827.0     Fc  \n",
       "8759       NaN    NaN      NaN    NaN  \n",
       "8901   13603.0     Fc   9355.0     Fc  \n",
       "9083   10901.0     Fc  11531.0     Fc  \n",
       "9367   11320.0     Fc  17564.0     Fc  \n",
       "9543   19983.0     Fc  20193.0     Fc  \n",
       "9812   14180.0     Fc  16780.0     Fc  \n",
       "9860    5429.0     Fc   5429.0     Fc  \n",
       "9992   25324.0     Fc  27554.0     Fc  \n",
       "10256   6694.0     Fc   5408.0     Fc  \n",
       "\n",
       "[56 rows x 108 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crp=pd.read_csv(\"C:/Users/dylan/Downloads/Production_Crops_E_Africa.csv\",encoding = \"ISO-8859-1\")\n",
    "#df_crp=df_crp.drop(['Item Code'], axis=1)\n",
    "df_crp=df_crp.drop(['Unit'], axis=1)\n",
    "df_crp=df_crp.drop(['Element Code'], axis=1)\n",
    "df_crp=df_crp.drop(['Area'], axis=1)\n",
    "df_crp=df_crp.drop(['Area Code'], axis=1)\n",
    "df_crp=df_crp.drop(['Item Code'], axis=1)\n",
    "dr_crp=df_crp.transpose()\n",
    "df_crp=df_crp[df_crp['Element'].str.contains(\"Yield\")]\n",
    "#df_crp=df_crp[df_crp['Element'].str.contains(\"Production\")]\n",
    "df_crp=df_crp[df_crp['Item'].str.contains(\"Cereals,Total\")]\n",
    "df_crp=df_crp.drop(['Element'], axis=1)\n",
    "df_crp=df_crp.drop(['Item'], axis=1)\n",
    "#df_crp=df_crp.drop(['Element'], axis=1)\n",
    "\n",
    "\n",
    "df_crp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3462.0</td>\n",
       "      <td>3462.0</td>\n",
       "      <td>8280.0</td>\n",
       "      <td>8280.0</td>\n",
       "      <td>5461.0</td>\n",
       "      <td>5461.0</td>\n",
       "      <td>3418.0</td>\n",
       "      <td>3418.0</td>\n",
       "      <td>4085.0</td>\n",
       "      <td>4085.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9023.0</td>\n",
       "      <td>9023.0</td>\n",
       "      <td>8057.0</td>\n",
       "      <td>8057.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8222.0</td>\n",
       "      <td>8222.0</td>\n",
       "      <td>9197.0</td>\n",
       "      <td>9197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8140.0</td>\n",
       "      <td>8140.0</td>\n",
       "      <td>8303.0</td>\n",
       "      <td>8303.0</td>\n",
       "      <td>4899.0</td>\n",
       "      <td>4899.0</td>\n",
       "      <td>3396.0</td>\n",
       "      <td>3396.0</td>\n",
       "      <td>4779.0</td>\n",
       "      <td>4779.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9037.0</td>\n",
       "      <td>9037.0</td>\n",
       "      <td>8313.0</td>\n",
       "      <td>8313.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8014.0</td>\n",
       "      <td>8014.0</td>\n",
       "      <td>9059.0</td>\n",
       "      <td>9059.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8038.0</td>\n",
       "      <td>8038.0</td>\n",
       "      <td>7984.0</td>\n",
       "      <td>7984.0</td>\n",
       "      <td>5035.0</td>\n",
       "      <td>5035.0</td>\n",
       "      <td>3828.0</td>\n",
       "      <td>3828.0</td>\n",
       "      <td>4731.0</td>\n",
       "      <td>4731.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9266.0</td>\n",
       "      <td>9266.0</td>\n",
       "      <td>9586.0</td>\n",
       "      <td>9586.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7069.0</td>\n",
       "      <td>7069.0</td>\n",
       "      <td>8225.0</td>\n",
       "      <td>8225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5302.0</td>\n",
       "      <td>5302.0</td>\n",
       "      <td>8758.0</td>\n",
       "      <td>8758.0</td>\n",
       "      <td>5293.0</td>\n",
       "      <td>5293.0</td>\n",
       "      <td>2759.0</td>\n",
       "      <td>2759.0</td>\n",
       "      <td>5225.0</td>\n",
       "      <td>5225.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8844.0</td>\n",
       "      <td>8844.0</td>\n",
       "      <td>8044.0</td>\n",
       "      <td>8044.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7889.0</td>\n",
       "      <td>7889.0</td>\n",
       "      <td>8205.0</td>\n",
       "      <td>8205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6063.0</td>\n",
       "      <td>6063.0</td>\n",
       "      <td>9320.0</td>\n",
       "      <td>9320.0</td>\n",
       "      <td>5515.0</td>\n",
       "      <td>5515.0</td>\n",
       "      <td>3298.0</td>\n",
       "      <td>3298.0</td>\n",
       "      <td>4857.0</td>\n",
       "      <td>4857.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9105.0</td>\n",
       "      <td>9105.0</td>\n",
       "      <td>8014.0</td>\n",
       "      <td>8014.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8235.0</td>\n",
       "      <td>8235.0</td>\n",
       "      <td>9308.0</td>\n",
       "      <td>9308.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4039.0</td>\n",
       "      <td>4039.0</td>\n",
       "      <td>8243.0</td>\n",
       "      <td>8243.0</td>\n",
       "      <td>5318.0</td>\n",
       "      <td>5318.0</td>\n",
       "      <td>4399.0</td>\n",
       "      <td>4399.0</td>\n",
       "      <td>4954.0</td>\n",
       "      <td>4954.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9089.0</td>\n",
       "      <td>9089.0</td>\n",
       "      <td>7602.0</td>\n",
       "      <td>7602.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8780.0</td>\n",
       "      <td>8780.0</td>\n",
       "      <td>9378.0</td>\n",
       "      <td>9378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6314.0</td>\n",
       "      <td>6314.0</td>\n",
       "      <td>8146.0</td>\n",
       "      <td>8146.0</td>\n",
       "      <td>5775.0</td>\n",
       "      <td>5775.0</td>\n",
       "      <td>3169.0</td>\n",
       "      <td>3169.0</td>\n",
       "      <td>4523.0</td>\n",
       "      <td>4523.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9131.0</td>\n",
       "      <td>9131.0</td>\n",
       "      <td>7025.0</td>\n",
       "      <td>7025.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8696.0</td>\n",
       "      <td>8696.0</td>\n",
       "      <td>12945.0</td>\n",
       "      <td>12945.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7052.0</td>\n",
       "      <td>7052.0</td>\n",
       "      <td>8057.0</td>\n",
       "      <td>8057.0</td>\n",
       "      <td>5738.0</td>\n",
       "      <td>5738.0</td>\n",
       "      <td>2616.0</td>\n",
       "      <td>2616.0</td>\n",
       "      <td>6258.0</td>\n",
       "      <td>6258.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10745.0</td>\n",
       "      <td>10745.0</td>\n",
       "      <td>5811.0</td>\n",
       "      <td>5811.0</td>\n",
       "      <td>7032.0</td>\n",
       "      <td>7032.0</td>\n",
       "      <td>7852.0</td>\n",
       "      <td>7852.0</td>\n",
       "      <td>10202.0</td>\n",
       "      <td>10202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6149.0</td>\n",
       "      <td>6149.0</td>\n",
       "      <td>8989.0</td>\n",
       "      <td>8989.0</td>\n",
       "      <td>5568.0</td>\n",
       "      <td>5568.0</td>\n",
       "      <td>3096.0</td>\n",
       "      <td>3096.0</td>\n",
       "      <td>4845.0</td>\n",
       "      <td>4845.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11855.0</td>\n",
       "      <td>11855.0</td>\n",
       "      <td>6385.0</td>\n",
       "      <td>6385.0</td>\n",
       "      <td>6019.0</td>\n",
       "      <td>6019.0</td>\n",
       "      <td>7740.0</td>\n",
       "      <td>7740.0</td>\n",
       "      <td>11453.0</td>\n",
       "      <td>11453.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6375.0</td>\n",
       "      <td>6375.0</td>\n",
       "      <td>9113.0</td>\n",
       "      <td>9113.0</td>\n",
       "      <td>5771.0</td>\n",
       "      <td>5771.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>632.0</td>\n",
       "      <td>5166.0</td>\n",
       "      <td>5166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13626.0</td>\n",
       "      <td>13626.0</td>\n",
       "      <td>5738.0</td>\n",
       "      <td>5738.0</td>\n",
       "      <td>5986.0</td>\n",
       "      <td>5986.0</td>\n",
       "      <td>6244.0</td>\n",
       "      <td>6244.0</td>\n",
       "      <td>9423.0</td>\n",
       "      <td>9423.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5878.0</td>\n",
       "      <td>5878.0</td>\n",
       "      <td>8057.0</td>\n",
       "      <td>8057.0</td>\n",
       "      <td>5327.0</td>\n",
       "      <td>5327.0</td>\n",
       "      <td>4480.0</td>\n",
       "      <td>4480.0</td>\n",
       "      <td>4445.0</td>\n",
       "      <td>4445.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10879.0</td>\n",
       "      <td>10879.0</td>\n",
       "      <td>7081.0</td>\n",
       "      <td>7081.0</td>\n",
       "      <td>6479.0</td>\n",
       "      <td>6479.0</td>\n",
       "      <td>8763.0</td>\n",
       "      <td>8763.0</td>\n",
       "      <td>13919.0</td>\n",
       "      <td>13919.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6717.0</td>\n",
       "      <td>6717.0</td>\n",
       "      <td>7803.0</td>\n",
       "      <td>7803.0</td>\n",
       "      <td>5558.0</td>\n",
       "      <td>5558.0</td>\n",
       "      <td>9485.0</td>\n",
       "      <td>9485.0</td>\n",
       "      <td>4612.0</td>\n",
       "      <td>4612.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12276.0</td>\n",
       "      <td>12276.0</td>\n",
       "      <td>6680.0</td>\n",
       "      <td>6680.0</td>\n",
       "      <td>6127.0</td>\n",
       "      <td>6127.0</td>\n",
       "      <td>10071.0</td>\n",
       "      <td>10071.0</td>\n",
       "      <td>16370.0</td>\n",
       "      <td>16370.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4937.0</td>\n",
       "      <td>4937.0</td>\n",
       "      <td>7640.0</td>\n",
       "      <td>7640.0</td>\n",
       "      <td>6877.0</td>\n",
       "      <td>6877.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>4366.0</td>\n",
       "      <td>4366.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11735.0</td>\n",
       "      <td>11735.0</td>\n",
       "      <td>9030.0</td>\n",
       "      <td>9030.0</td>\n",
       "      <td>6690.0</td>\n",
       "      <td>6690.0</td>\n",
       "      <td>8387.0</td>\n",
       "      <td>8387.0</td>\n",
       "      <td>9613.0</td>\n",
       "      <td>9613.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4979.0</td>\n",
       "      <td>4979.0</td>\n",
       "      <td>7126.0</td>\n",
       "      <td>7126.0</td>\n",
       "      <td>7079.0</td>\n",
       "      <td>7079.0</td>\n",
       "      <td>6910.0</td>\n",
       "      <td>6910.0</td>\n",
       "      <td>4882.0</td>\n",
       "      <td>4882.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10798.0</td>\n",
       "      <td>10798.0</td>\n",
       "      <td>7646.0</td>\n",
       "      <td>7646.0</td>\n",
       "      <td>6667.0</td>\n",
       "      <td>6667.0</td>\n",
       "      <td>9677.0</td>\n",
       "      <td>9677.0</td>\n",
       "      <td>14855.0</td>\n",
       "      <td>14855.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8452.0</td>\n",
       "      <td>8452.0</td>\n",
       "      <td>7739.0</td>\n",
       "      <td>7739.0</td>\n",
       "      <td>7393.0</td>\n",
       "      <td>7393.0</td>\n",
       "      <td>4462.0</td>\n",
       "      <td>4462.0</td>\n",
       "      <td>5581.0</td>\n",
       "      <td>5581.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13508.0</td>\n",
       "      <td>13508.0</td>\n",
       "      <td>10522.0</td>\n",
       "      <td>10522.0</td>\n",
       "      <td>6667.0</td>\n",
       "      <td>6667.0</td>\n",
       "      <td>13111.0</td>\n",
       "      <td>13111.0</td>\n",
       "      <td>13602.0</td>\n",
       "      <td>13602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6905.0</td>\n",
       "      <td>6905.0</td>\n",
       "      <td>7755.0</td>\n",
       "      <td>7755.0</td>\n",
       "      <td>7246.0</td>\n",
       "      <td>7246.0</td>\n",
       "      <td>6432.0</td>\n",
       "      <td>6432.0</td>\n",
       "      <td>4994.0</td>\n",
       "      <td>4994.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12172.0</td>\n",
       "      <td>12172.0</td>\n",
       "      <td>10076.0</td>\n",
       "      <td>10076.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>13757.0</td>\n",
       "      <td>13757.0</td>\n",
       "      <td>13941.0</td>\n",
       "      <td>13941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4109.0</td>\n",
       "      <td>4109.0</td>\n",
       "      <td>6031.0</td>\n",
       "      <td>6031.0</td>\n",
       "      <td>7540.0</td>\n",
       "      <td>7540.0</td>\n",
       "      <td>4151.0</td>\n",
       "      <td>4151.0</td>\n",
       "      <td>5288.0</td>\n",
       "      <td>5288.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12054.0</td>\n",
       "      <td>12054.0</td>\n",
       "      <td>10059.0</td>\n",
       "      <td>10059.0</td>\n",
       "      <td>7059.0</td>\n",
       "      <td>7059.0</td>\n",
       "      <td>15047.0</td>\n",
       "      <td>15047.0</td>\n",
       "      <td>14759.0</td>\n",
       "      <td>14759.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5861.0</td>\n",
       "      <td>5861.0</td>\n",
       "      <td>6671.0</td>\n",
       "      <td>6671.0</td>\n",
       "      <td>7411.0</td>\n",
       "      <td>7411.0</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>2687.0</td>\n",
       "      <td>5764.0</td>\n",
       "      <td>5764.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12101.0</td>\n",
       "      <td>12101.0</td>\n",
       "      <td>9996.0</td>\n",
       "      <td>9996.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>15154.0</td>\n",
       "      <td>15154.0</td>\n",
       "      <td>13789.0</td>\n",
       "      <td>13789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5615.0</td>\n",
       "      <td>5615.0</td>\n",
       "      <td>5092.0</td>\n",
       "      <td>5092.0</td>\n",
       "      <td>7209.0</td>\n",
       "      <td>7209.0</td>\n",
       "      <td>1323.0</td>\n",
       "      <td>1323.0</td>\n",
       "      <td>5801.0</td>\n",
       "      <td>5801.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16137.0</td>\n",
       "      <td>16137.0</td>\n",
       "      <td>11663.0</td>\n",
       "      <td>11663.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>15291.0</td>\n",
       "      <td>15291.0</td>\n",
       "      <td>11442.0</td>\n",
       "      <td>11442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7604.0</td>\n",
       "      <td>7604.0</td>\n",
       "      <td>6179.0</td>\n",
       "      <td>6179.0</td>\n",
       "      <td>7183.0</td>\n",
       "      <td>7183.0</td>\n",
       "      <td>2372.0</td>\n",
       "      <td>2372.0</td>\n",
       "      <td>5698.0</td>\n",
       "      <td>5698.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14910.0</td>\n",
       "      <td>14910.0</td>\n",
       "      <td>10202.0</td>\n",
       "      <td>10202.0</td>\n",
       "      <td>6522.0</td>\n",
       "      <td>6522.0</td>\n",
       "      <td>15675.0</td>\n",
       "      <td>15675.0</td>\n",
       "      <td>11861.0</td>\n",
       "      <td>11861.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6462.0</td>\n",
       "      <td>6462.0</td>\n",
       "      <td>4511.0</td>\n",
       "      <td>4511.0</td>\n",
       "      <td>6538.0</td>\n",
       "      <td>6538.0</td>\n",
       "      <td>2752.0</td>\n",
       "      <td>2752.0</td>\n",
       "      <td>5766.0</td>\n",
       "      <td>5766.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15617.0</td>\n",
       "      <td>15617.0</td>\n",
       "      <td>10031.0</td>\n",
       "      <td>10031.0</td>\n",
       "      <td>6957.0</td>\n",
       "      <td>6957.0</td>\n",
       "      <td>19306.0</td>\n",
       "      <td>19306.0</td>\n",
       "      <td>17496.0</td>\n",
       "      <td>17496.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5933.0</td>\n",
       "      <td>5933.0</td>\n",
       "      <td>4506.0</td>\n",
       "      <td>4506.0</td>\n",
       "      <td>6495.0</td>\n",
       "      <td>6495.0</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>2879.0</td>\n",
       "      <td>5683.0</td>\n",
       "      <td>5683.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13090.0</td>\n",
       "      <td>13090.0</td>\n",
       "      <td>14295.0</td>\n",
       "      <td>14295.0</td>\n",
       "      <td>6957.0</td>\n",
       "      <td>6957.0</td>\n",
       "      <td>15721.0</td>\n",
       "      <td>15721.0</td>\n",
       "      <td>11396.0</td>\n",
       "      <td>11396.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5795.0</td>\n",
       "      <td>5795.0</td>\n",
       "      <td>4846.0</td>\n",
       "      <td>4846.0</td>\n",
       "      <td>5999.0</td>\n",
       "      <td>5999.0</td>\n",
       "      <td>2653.0</td>\n",
       "      <td>2653.0</td>\n",
       "      <td>5149.0</td>\n",
       "      <td>5149.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16173.0</td>\n",
       "      <td>16173.0</td>\n",
       "      <td>13009.0</td>\n",
       "      <td>13009.0</td>\n",
       "      <td>7083.0</td>\n",
       "      <td>7083.0</td>\n",
       "      <td>16512.0</td>\n",
       "      <td>16512.0</td>\n",
       "      <td>6336.0</td>\n",
       "      <td>6336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5474.0</td>\n",
       "      <td>5474.0</td>\n",
       "      <td>4660.0</td>\n",
       "      <td>4660.0</td>\n",
       "      <td>7991.0</td>\n",
       "      <td>7991.0</td>\n",
       "      <td>1507.0</td>\n",
       "      <td>1507.0</td>\n",
       "      <td>5899.0</td>\n",
       "      <td>5899.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10419.0</td>\n",
       "      <td>10419.0</td>\n",
       "      <td>12707.0</td>\n",
       "      <td>12707.0</td>\n",
       "      <td>7417.0</td>\n",
       "      <td>7417.0</td>\n",
       "      <td>16461.0</td>\n",
       "      <td>16461.0</td>\n",
       "      <td>7904.0</td>\n",
       "      <td>7904.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9125.0</td>\n",
       "      <td>9125.0</td>\n",
       "      <td>4491.0</td>\n",
       "      <td>4491.0</td>\n",
       "      <td>8558.0</td>\n",
       "      <td>8558.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>7088.0</td>\n",
       "      <td>7088.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14695.0</td>\n",
       "      <td>14695.0</td>\n",
       "      <td>13668.0</td>\n",
       "      <td>13668.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>18428.0</td>\n",
       "      <td>18428.0</td>\n",
       "      <td>18688.0</td>\n",
       "      <td>18688.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8372.0</td>\n",
       "      <td>8372.0</td>\n",
       "      <td>3798.0</td>\n",
       "      <td>3798.0</td>\n",
       "      <td>8362.0</td>\n",
       "      <td>8362.0</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>6995.0</td>\n",
       "      <td>6995.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11813.0</td>\n",
       "      <td>11813.0</td>\n",
       "      <td>11266.0</td>\n",
       "      <td>11266.0</td>\n",
       "      <td>7308.0</td>\n",
       "      <td>7308.0</td>\n",
       "      <td>19306.0</td>\n",
       "      <td>19306.0</td>\n",
       "      <td>16943.0</td>\n",
       "      <td>16943.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7599.0</td>\n",
       "      <td>7599.0</td>\n",
       "      <td>3812.0</td>\n",
       "      <td>3812.0</td>\n",
       "      <td>7251.0</td>\n",
       "      <td>7251.0</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>1907.0</td>\n",
       "      <td>6417.0</td>\n",
       "      <td>6417.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14275.0</td>\n",
       "      <td>14275.0</td>\n",
       "      <td>12743.0</td>\n",
       "      <td>12743.0</td>\n",
       "      <td>7692.0</td>\n",
       "      <td>7692.0</td>\n",
       "      <td>16103.0</td>\n",
       "      <td>16103.0</td>\n",
       "      <td>8538.0</td>\n",
       "      <td>8538.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5741.0</td>\n",
       "      <td>5741.0</td>\n",
       "      <td>3324.0</td>\n",
       "      <td>3324.0</td>\n",
       "      <td>8347.0</td>\n",
       "      <td>8347.0</td>\n",
       "      <td>3756.0</td>\n",
       "      <td>3756.0</td>\n",
       "      <td>7269.0</td>\n",
       "      <td>7269.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14383.0</td>\n",
       "      <td>14383.0</td>\n",
       "      <td>12201.0</td>\n",
       "      <td>12201.0</td>\n",
       "      <td>7407.0</td>\n",
       "      <td>7407.0</td>\n",
       "      <td>24574.0</td>\n",
       "      <td>24574.0</td>\n",
       "      <td>15914.0</td>\n",
       "      <td>15914.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7604.0</td>\n",
       "      <td>7604.0</td>\n",
       "      <td>2774.0</td>\n",
       "      <td>2774.0</td>\n",
       "      <td>8537.0</td>\n",
       "      <td>8537.0</td>\n",
       "      <td>2889.0</td>\n",
       "      <td>2889.0</td>\n",
       "      <td>6729.0</td>\n",
       "      <td>6729.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15158.0</td>\n",
       "      <td>15158.0</td>\n",
       "      <td>14892.0</td>\n",
       "      <td>14892.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>7500.0</td>\n",
       "      <td>17173.0</td>\n",
       "      <td>17173.0</td>\n",
       "      <td>15096.0</td>\n",
       "      <td>15096.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6877.0</td>\n",
       "      <td>6877.0</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>8479.0</td>\n",
       "      <td>8479.0</td>\n",
       "      <td>2655.0</td>\n",
       "      <td>2655.0</td>\n",
       "      <td>6002.0</td>\n",
       "      <td>6002.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14976.0</td>\n",
       "      <td>14976.0</td>\n",
       "      <td>15064.0</td>\n",
       "      <td>15064.0</td>\n",
       "      <td>7586.0</td>\n",
       "      <td>7586.0</td>\n",
       "      <td>13520.0</td>\n",
       "      <td>13520.0</td>\n",
       "      <td>16254.0</td>\n",
       "      <td>16254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>11146.0</td>\n",
       "      <td>11146.0</td>\n",
       "      <td>4175.0</td>\n",
       "      <td>4175.0</td>\n",
       "      <td>8786.0</td>\n",
       "      <td>8786.0</td>\n",
       "      <td>3812.0</td>\n",
       "      <td>3812.0</td>\n",
       "      <td>8766.0</td>\n",
       "      <td>8766.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14341.0</td>\n",
       "      <td>14341.0</td>\n",
       "      <td>12340.0</td>\n",
       "      <td>12340.0</td>\n",
       "      <td>7667.0</td>\n",
       "      <td>7667.0</td>\n",
       "      <td>16403.0</td>\n",
       "      <td>16403.0</td>\n",
       "      <td>13372.0</td>\n",
       "      <td>13372.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>9433.0</td>\n",
       "      <td>9433.0</td>\n",
       "      <td>3971.0</td>\n",
       "      <td>3971.0</td>\n",
       "      <td>9134.0</td>\n",
       "      <td>9134.0</td>\n",
       "      <td>2887.0</td>\n",
       "      <td>2887.0</td>\n",
       "      <td>8715.0</td>\n",
       "      <td>8715.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15301.0</td>\n",
       "      <td>15301.0</td>\n",
       "      <td>10875.0</td>\n",
       "      <td>10875.0</td>\n",
       "      <td>7760.0</td>\n",
       "      <td>7760.0</td>\n",
       "      <td>7631.0</td>\n",
       "      <td>7631.0</td>\n",
       "      <td>4126.0</td>\n",
       "      <td>4126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7417.0</td>\n",
       "      <td>7417.0</td>\n",
       "      <td>2680.0</td>\n",
       "      <td>2680.0</td>\n",
       "      <td>9192.0</td>\n",
       "      <td>9192.0</td>\n",
       "      <td>4241.0</td>\n",
       "      <td>4241.0</td>\n",
       "      <td>9362.0</td>\n",
       "      <td>9362.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15410.0</td>\n",
       "      <td>15410.0</td>\n",
       "      <td>12274.0</td>\n",
       "      <td>12274.0</td>\n",
       "      <td>7742.0</td>\n",
       "      <td>7742.0</td>\n",
       "      <td>23029.0</td>\n",
       "      <td>23029.0</td>\n",
       "      <td>14785.0</td>\n",
       "      <td>14785.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7494.0</td>\n",
       "      <td>7494.0</td>\n",
       "      <td>2984.0</td>\n",
       "      <td>2984.0</td>\n",
       "      <td>9569.0</td>\n",
       "      <td>9569.0</td>\n",
       "      <td>2237.0</td>\n",
       "      <td>2237.0</td>\n",
       "      <td>7879.0</td>\n",
       "      <td>7879.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14950.0</td>\n",
       "      <td>14950.0</td>\n",
       "      <td>11517.0</td>\n",
       "      <td>11517.0</td>\n",
       "      <td>7758.0</td>\n",
       "      <td>7758.0</td>\n",
       "      <td>14175.0</td>\n",
       "      <td>14175.0</td>\n",
       "      <td>14725.0</td>\n",
       "      <td>14725.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8295.0</td>\n",
       "      <td>8295.0</td>\n",
       "      <td>4015.0</td>\n",
       "      <td>4015.0</td>\n",
       "      <td>10865.0</td>\n",
       "      <td>10865.0</td>\n",
       "      <td>4583.0</td>\n",
       "      <td>4583.0</td>\n",
       "      <td>8511.0</td>\n",
       "      <td>8511.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15712.0</td>\n",
       "      <td>15712.0</td>\n",
       "      <td>17028.0</td>\n",
       "      <td>17028.0</td>\n",
       "      <td>7813.0</td>\n",
       "      <td>7813.0</td>\n",
       "      <td>13314.0</td>\n",
       "      <td>13314.0</td>\n",
       "      <td>5374.0</td>\n",
       "      <td>5374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>13378.0</td>\n",
       "      <td>13378.0</td>\n",
       "      <td>6531.0</td>\n",
       "      <td>6531.0</td>\n",
       "      <td>10096.0</td>\n",
       "      <td>10096.0</td>\n",
       "      <td>3233.0</td>\n",
       "      <td>3233.0</td>\n",
       "      <td>9165.0</td>\n",
       "      <td>9165.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12049.0</td>\n",
       "      <td>12049.0</td>\n",
       "      <td>15881.0</td>\n",
       "      <td>15881.0</td>\n",
       "      <td>7755.0</td>\n",
       "      <td>7755.0</td>\n",
       "      <td>19117.0</td>\n",
       "      <td>19117.0</td>\n",
       "      <td>15237.0</td>\n",
       "      <td>15237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>7798.0</td>\n",
       "      <td>7798.0</td>\n",
       "      <td>5673.0</td>\n",
       "      <td>5673.0</td>\n",
       "      <td>11170.0</td>\n",
       "      <td>11170.0</td>\n",
       "      <td>2767.0</td>\n",
       "      <td>2767.0</td>\n",
       "      <td>7045.0</td>\n",
       "      <td>7045.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12181.0</td>\n",
       "      <td>12181.0</td>\n",
       "      <td>11012.0</td>\n",
       "      <td>11012.0</td>\n",
       "      <td>7879.0</td>\n",
       "      <td>7879.0</td>\n",
       "      <td>14134.0</td>\n",
       "      <td>14134.0</td>\n",
       "      <td>12679.0</td>\n",
       "      <td>12679.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>8463.0</td>\n",
       "      <td>8463.0</td>\n",
       "      <td>7016.0</td>\n",
       "      <td>7016.0</td>\n",
       "      <td>10559.0</td>\n",
       "      <td>10559.0</td>\n",
       "      <td>2467.0</td>\n",
       "      <td>2467.0</td>\n",
       "      <td>8890.0</td>\n",
       "      <td>8890.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15264.0</td>\n",
       "      <td>15264.0</td>\n",
       "      <td>11987.0</td>\n",
       "      <td>11987.0</td>\n",
       "      <td>7756.0</td>\n",
       "      <td>7756.0</td>\n",
       "      <td>12112.0</td>\n",
       "      <td>12112.0</td>\n",
       "      <td>11103.0</td>\n",
       "      <td>11103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10699.0</td>\n",
       "      <td>10699.0</td>\n",
       "      <td>6204.0</td>\n",
       "      <td>6204.0</td>\n",
       "      <td>11626.0</td>\n",
       "      <td>11626.0</td>\n",
       "      <td>2291.0</td>\n",
       "      <td>2291.0</td>\n",
       "      <td>9129.0</td>\n",
       "      <td>9129.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16339.0</td>\n",
       "      <td>16339.0</td>\n",
       "      <td>17664.0</td>\n",
       "      <td>17664.0</td>\n",
       "      <td>7941.0</td>\n",
       "      <td>7941.0</td>\n",
       "      <td>13236.0</td>\n",
       "      <td>13236.0</td>\n",
       "      <td>10634.0</td>\n",
       "      <td>10634.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>8833.0</td>\n",
       "      <td>8833.0</td>\n",
       "      <td>5644.0</td>\n",
       "      <td>5644.0</td>\n",
       "      <td>11021.0</td>\n",
       "      <td>11021.0</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>8590.0</td>\n",
       "      <td>8590.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15394.0</td>\n",
       "      <td>15394.0</td>\n",
       "      <td>14406.0</td>\n",
       "      <td>14406.0</td>\n",
       "      <td>7692.0</td>\n",
       "      <td>7692.0</td>\n",
       "      <td>16823.0</td>\n",
       "      <td>16823.0</td>\n",
       "      <td>14045.0</td>\n",
       "      <td>14045.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>11069.0</td>\n",
       "      <td>11069.0</td>\n",
       "      <td>5854.0</td>\n",
       "      <td>5854.0</td>\n",
       "      <td>10693.0</td>\n",
       "      <td>10693.0</td>\n",
       "      <td>5544.0</td>\n",
       "      <td>5544.0</td>\n",
       "      <td>9678.0</td>\n",
       "      <td>9678.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16411.0</td>\n",
       "      <td>16411.0</td>\n",
       "      <td>20435.0</td>\n",
       "      <td>20435.0</td>\n",
       "      <td>7540.0</td>\n",
       "      <td>7540.0</td>\n",
       "      <td>14021.0</td>\n",
       "      <td>14021.0</td>\n",
       "      <td>11596.0</td>\n",
       "      <td>11596.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10586.0</td>\n",
       "      <td>10586.0</td>\n",
       "      <td>6272.0</td>\n",
       "      <td>6272.0</td>\n",
       "      <td>9451.0</td>\n",
       "      <td>9451.0</td>\n",
       "      <td>3577.0</td>\n",
       "      <td>3577.0</td>\n",
       "      <td>9427.0</td>\n",
       "      <td>9427.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16388.0</td>\n",
       "      <td>16388.0</td>\n",
       "      <td>18995.0</td>\n",
       "      <td>18995.0</td>\n",
       "      <td>7478.0</td>\n",
       "      <td>7478.0</td>\n",
       "      <td>14192.0</td>\n",
       "      <td>14192.0</td>\n",
       "      <td>5470.0</td>\n",
       "      <td>5470.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>14703.0</td>\n",
       "      <td>14703.0</td>\n",
       "      <td>6461.0</td>\n",
       "      <td>6461.0</td>\n",
       "      <td>11487.0</td>\n",
       "      <td>11487.0</td>\n",
       "      <td>12146.0</td>\n",
       "      <td>12146.0</td>\n",
       "      <td>9963.0</td>\n",
       "      <td>9963.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16776.0</td>\n",
       "      <td>16776.0</td>\n",
       "      <td>8581.0</td>\n",
       "      <td>8581.0</td>\n",
       "      <td>7420.0</td>\n",
       "      <td>7420.0</td>\n",
       "      <td>17019.0</td>\n",
       "      <td>17019.0</td>\n",
       "      <td>8033.0</td>\n",
       "      <td>8033.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>13441.0</td>\n",
       "      <td>13441.0</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>4917.0</td>\n",
       "      <td>11466.0</td>\n",
       "      <td>11466.0</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>2750.0</td>\n",
       "      <td>9407.0</td>\n",
       "      <td>9407.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14680.0</td>\n",
       "      <td>14680.0</td>\n",
       "      <td>13706.0</td>\n",
       "      <td>13706.0</td>\n",
       "      <td>7600.0</td>\n",
       "      <td>7600.0</td>\n",
       "      <td>18144.0</td>\n",
       "      <td>18144.0</td>\n",
       "      <td>10753.0</td>\n",
       "      <td>10753.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>15008.0</td>\n",
       "      <td>15008.0</td>\n",
       "      <td>5834.0</td>\n",
       "      <td>5834.0</td>\n",
       "      <td>11364.0</td>\n",
       "      <td>11364.0</td>\n",
       "      <td>4391.0</td>\n",
       "      <td>4391.0</td>\n",
       "      <td>11272.0</td>\n",
       "      <td>11272.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15738.0</td>\n",
       "      <td>15738.0</td>\n",
       "      <td>11016.0</td>\n",
       "      <td>11016.0</td>\n",
       "      <td>7345.0</td>\n",
       "      <td>7345.0</td>\n",
       "      <td>18987.0</td>\n",
       "      <td>18987.0</td>\n",
       "      <td>5883.0</td>\n",
       "      <td>5883.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>15036.0</td>\n",
       "      <td>15036.0</td>\n",
       "      <td>4459.0</td>\n",
       "      <td>4459.0</td>\n",
       "      <td>11257.0</td>\n",
       "      <td>11257.0</td>\n",
       "      <td>3728.0</td>\n",
       "      <td>3728.0</td>\n",
       "      <td>12039.0</td>\n",
       "      <td>12039.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15229.0</td>\n",
       "      <td>15229.0</td>\n",
       "      <td>13267.0</td>\n",
       "      <td>13267.0</td>\n",
       "      <td>7274.0</td>\n",
       "      <td>7274.0</td>\n",
       "      <td>18160.0</td>\n",
       "      <td>18160.0</td>\n",
       "      <td>8469.0</td>\n",
       "      <td>8469.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>12535.0</td>\n",
       "      <td>12535.0</td>\n",
       "      <td>4643.0</td>\n",
       "      <td>4643.0</td>\n",
       "      <td>10136.0</td>\n",
       "      <td>10136.0</td>\n",
       "      <td>6392.0</td>\n",
       "      <td>6392.0</td>\n",
       "      <td>9361.0</td>\n",
       "      <td>9361.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15260.0</td>\n",
       "      <td>15260.0</td>\n",
       "      <td>14273.0</td>\n",
       "      <td>14273.0</td>\n",
       "      <td>7692.0</td>\n",
       "      <td>7692.0</td>\n",
       "      <td>22526.0</td>\n",
       "      <td>22526.0</td>\n",
       "      <td>7448.0</td>\n",
       "      <td>7448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>10340.0</td>\n",
       "      <td>10340.0</td>\n",
       "      <td>6527.0</td>\n",
       "      <td>6527.0</td>\n",
       "      <td>12480.0</td>\n",
       "      <td>12480.0</td>\n",
       "      <td>3616.0</td>\n",
       "      <td>3616.0</td>\n",
       "      <td>10399.0</td>\n",
       "      <td>10399.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20558.0</td>\n",
       "      <td>20558.0</td>\n",
       "      <td>13339.0</td>\n",
       "      <td>13339.0</td>\n",
       "      <td>6905.0</td>\n",
       "      <td>6905.0</td>\n",
       "      <td>21798.0</td>\n",
       "      <td>21798.0</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>3100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>16539.0</td>\n",
       "      <td>16539.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>12712.0</td>\n",
       "      <td>12712.0</td>\n",
       "      <td>3640.0</td>\n",
       "      <td>3640.0</td>\n",
       "      <td>10020.0</td>\n",
       "      <td>10020.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20383.0</td>\n",
       "      <td>20383.0</td>\n",
       "      <td>11104.0</td>\n",
       "      <td>11104.0</td>\n",
       "      <td>6526.0</td>\n",
       "      <td>6526.0</td>\n",
       "      <td>20664.0</td>\n",
       "      <td>20664.0</td>\n",
       "      <td>4520.0</td>\n",
       "      <td>4520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>14011.0</td>\n",
       "      <td>14011.0</td>\n",
       "      <td>6293.0</td>\n",
       "      <td>6293.0</td>\n",
       "      <td>12006.0</td>\n",
       "      <td>12006.0</td>\n",
       "      <td>3717.0</td>\n",
       "      <td>3717.0</td>\n",
       "      <td>10627.0</td>\n",
       "      <td>10627.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19784.0</td>\n",
       "      <td>19784.0</td>\n",
       "      <td>16479.0</td>\n",
       "      <td>16479.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>5714.0</td>\n",
       "      <td>25335.0</td>\n",
       "      <td>25335.0</td>\n",
       "      <td>7346.0</td>\n",
       "      <td>7346.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>14424.0</td>\n",
       "      <td>14424.0</td>\n",
       "      <td>6624.0</td>\n",
       "      <td>6624.0</td>\n",
       "      <td>15178.0</td>\n",
       "      <td>15178.0</td>\n",
       "      <td>4523.0</td>\n",
       "      <td>4523.0</td>\n",
       "      <td>9951.0</td>\n",
       "      <td>9951.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20776.0</td>\n",
       "      <td>20776.0</td>\n",
       "      <td>13904.0</td>\n",
       "      <td>13904.0</td>\n",
       "      <td>5917.0</td>\n",
       "      <td>5917.0</td>\n",
       "      <td>27314.0</td>\n",
       "      <td>27314.0</td>\n",
       "      <td>5932.0</td>\n",
       "      <td>5932.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>16776.0</td>\n",
       "      <td>16776.0</td>\n",
       "      <td>5520.0</td>\n",
       "      <td>5520.0</td>\n",
       "      <td>13732.0</td>\n",
       "      <td>13732.0</td>\n",
       "      <td>3672.0</td>\n",
       "      <td>3672.0</td>\n",
       "      <td>12030.0</td>\n",
       "      <td>12030.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20286.0</td>\n",
       "      <td>20286.0</td>\n",
       "      <td>13148.0</td>\n",
       "      <td>13148.0</td>\n",
       "      <td>5441.0</td>\n",
       "      <td>5441.0</td>\n",
       "      <td>26893.0</td>\n",
       "      <td>26893.0</td>\n",
       "      <td>6889.0</td>\n",
       "      <td>6889.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>18131.0</td>\n",
       "      <td>18131.0</td>\n",
       "      <td>8144.0</td>\n",
       "      <td>8144.0</td>\n",
       "      <td>13993.0</td>\n",
       "      <td>13993.0</td>\n",
       "      <td>2176.0</td>\n",
       "      <td>2176.0</td>\n",
       "      <td>11565.0</td>\n",
       "      <td>11565.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19983.0</td>\n",
       "      <td>19983.0</td>\n",
       "      <td>14180.0</td>\n",
       "      <td>14180.0</td>\n",
       "      <td>5429.0</td>\n",
       "      <td>5429.0</td>\n",
       "      <td>25324.0</td>\n",
       "      <td>25324.0</td>\n",
       "      <td>6694.0</td>\n",
       "      <td>6694.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>13692.0</td>\n",
       "      <td>13692.0</td>\n",
       "      <td>8883.0</td>\n",
       "      <td>8883.0</td>\n",
       "      <td>14603.0</td>\n",
       "      <td>14603.0</td>\n",
       "      <td>3044.0</td>\n",
       "      <td>3044.0</td>\n",
       "      <td>12258.0</td>\n",
       "      <td>12258.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20193.0</td>\n",
       "      <td>20193.0</td>\n",
       "      <td>16780.0</td>\n",
       "      <td>16780.0</td>\n",
       "      <td>5429.0</td>\n",
       "      <td>5429.0</td>\n",
       "      <td>27554.0</td>\n",
       "      <td>27554.0</td>\n",
       "      <td>5408.0</td>\n",
       "      <td>5408.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55 rows × 112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1       2       3        4        5        6        7    \\\n",
       "0      3462.0   3462.0  8280.0  8280.0   5461.0   5461.0   3418.0   3418.0   \n",
       "1      8140.0   8140.0  8303.0  8303.0   4899.0   4899.0   3396.0   3396.0   \n",
       "2      8038.0   8038.0  7984.0  7984.0   5035.0   5035.0   3828.0   3828.0   \n",
       "3      5302.0   5302.0  8758.0  8758.0   5293.0   5293.0   2759.0   2759.0   \n",
       "4      6063.0   6063.0  9320.0  9320.0   5515.0   5515.0   3298.0   3298.0   \n",
       "5      4039.0   4039.0  8243.0  8243.0   5318.0   5318.0   4399.0   4399.0   \n",
       "6      6314.0   6314.0  8146.0  8146.0   5775.0   5775.0   3169.0   3169.0   \n",
       "7      7052.0   7052.0  8057.0  8057.0   5738.0   5738.0   2616.0   2616.0   \n",
       "8      6149.0   6149.0  8989.0  8989.0   5568.0   5568.0   3096.0   3096.0   \n",
       "9      6375.0   6375.0  9113.0  9113.0   5771.0   5771.0    632.0    632.0   \n",
       "10     5878.0   5878.0  8057.0  8057.0   5327.0   5327.0   4480.0   4480.0   \n",
       "11     6717.0   6717.0  7803.0  7803.0   5558.0   5558.0   9485.0   9485.0   \n",
       "12     4937.0   4937.0  7640.0  7640.0   6877.0   6877.0   2756.0   2756.0   \n",
       "13     4979.0   4979.0  7126.0  7126.0   7079.0   7079.0   6910.0   6910.0   \n",
       "14     8452.0   8452.0  7739.0  7739.0   7393.0   7393.0   4462.0   4462.0   \n",
       "15     6905.0   6905.0  7755.0  7755.0   7246.0   7246.0   6432.0   6432.0   \n",
       "16     4109.0   4109.0  6031.0  6031.0   7540.0   7540.0   4151.0   4151.0   \n",
       "17     5861.0   5861.0  6671.0  6671.0   7411.0   7411.0   2687.0   2687.0   \n",
       "18     5615.0   5615.0  5092.0  5092.0   7209.0   7209.0   1323.0   1323.0   \n",
       "19     7604.0   7604.0  6179.0  6179.0   7183.0   7183.0   2372.0   2372.0   \n",
       "20     6462.0   6462.0  4511.0  4511.0   6538.0   6538.0   2752.0   2752.0   \n",
       "21     5933.0   5933.0  4506.0  4506.0   6495.0   6495.0   2879.0   2879.0   \n",
       "22     5795.0   5795.0  4846.0  4846.0   5999.0   5999.0   2653.0   2653.0   \n",
       "23     5474.0   5474.0  4660.0  4660.0   7991.0   7991.0   1507.0   1507.0   \n",
       "24     9125.0   9125.0  4491.0  4491.0   8558.0   8558.0   2007.0   2007.0   \n",
       "25     8372.0   8372.0  3798.0  3798.0   8362.0   8362.0   1985.0   1985.0   \n",
       "26     7599.0   7599.0  3812.0  3812.0   7251.0   7251.0   1907.0   1907.0   \n",
       "27     5741.0   5741.0  3324.0  3324.0   8347.0   8347.0   3756.0   3756.0   \n",
       "28     7604.0   7604.0  2774.0  2774.0   8537.0   8537.0   2889.0   2889.0   \n",
       "29     6877.0   6877.0  3210.0  3210.0   8479.0   8479.0   2655.0   2655.0   \n",
       "30    11146.0  11146.0  4175.0  4175.0   8786.0   8786.0   3812.0   3812.0   \n",
       "31     9433.0   9433.0  3971.0  3971.0   9134.0   9134.0   2887.0   2887.0   \n",
       "32     7417.0   7417.0  2680.0  2680.0   9192.0   9192.0   4241.0   4241.0   \n",
       "33     7494.0   7494.0  2984.0  2984.0   9569.0   9569.0   2237.0   2237.0   \n",
       "34     8295.0   8295.0  4015.0  4015.0  10865.0  10865.0   4583.0   4583.0   \n",
       "35    13378.0  13378.0  6531.0  6531.0  10096.0  10096.0   3233.0   3233.0   \n",
       "36     7798.0   7798.0  5673.0  5673.0  11170.0  11170.0   2767.0   2767.0   \n",
       "37     8463.0   8463.0  7016.0  7016.0  10559.0  10559.0   2467.0   2467.0   \n",
       "38    10699.0  10699.0  6204.0  6204.0  11626.0  11626.0   2291.0   2291.0   \n",
       "39     8833.0   8833.0  5644.0  5644.0  11021.0  11021.0   1529.0   1529.0   \n",
       "40    11069.0  11069.0  5854.0  5854.0  10693.0  10693.0   5544.0   5544.0   \n",
       "41    10586.0  10586.0  6272.0  6272.0   9451.0   9451.0   3577.0   3577.0   \n",
       "42    14703.0  14703.0  6461.0  6461.0  11487.0  11487.0  12146.0  12146.0   \n",
       "43    13441.0  13441.0  4917.0  4917.0  11466.0  11466.0   2750.0   2750.0   \n",
       "44    15008.0  15008.0  5834.0  5834.0  11364.0  11364.0   4391.0   4391.0   \n",
       "45    15036.0  15036.0  4459.0  4459.0  11257.0  11257.0   3728.0   3728.0   \n",
       "46    12535.0  12535.0  4643.0  4643.0  10136.0  10136.0   6392.0   6392.0   \n",
       "47    10340.0  10340.0  6527.0  6527.0  12480.0  12480.0   3616.0   3616.0   \n",
       "48    16539.0  16539.0  5714.0  5714.0  12712.0  12712.0   3640.0   3640.0   \n",
       "49    14011.0  14011.0  6293.0  6293.0  12006.0  12006.0   3717.0   3717.0   \n",
       "50    14424.0  14424.0  6624.0  6624.0  15178.0  15178.0   4523.0   4523.0   \n",
       "51    16776.0  16776.0  5520.0  5520.0  13732.0  13732.0   3672.0   3672.0   \n",
       "52    18131.0  18131.0  8144.0  8144.0  13993.0  13993.0   2176.0   2176.0   \n",
       "53    13692.0  13692.0  8883.0  8883.0  14603.0  14603.0   3044.0   3044.0   \n",
       "last      NaN      NaN     NaN     NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "          8        9    ...      102      103      104      105     106  \\\n",
       "0      4085.0   4085.0  ...   9023.0   9023.0   8057.0   8057.0     NaN   \n",
       "1      4779.0   4779.0  ...   9037.0   9037.0   8313.0   8313.0     NaN   \n",
       "2      4731.0   4731.0  ...   9266.0   9266.0   9586.0   9586.0     NaN   \n",
       "3      5225.0   5225.0  ...   8844.0   8844.0   8044.0   8044.0     NaN   \n",
       "4      4857.0   4857.0  ...   9105.0   9105.0   8014.0   8014.0     NaN   \n",
       "5      4954.0   4954.0  ...   9089.0   9089.0   7602.0   7602.0     NaN   \n",
       "6      4523.0   4523.0  ...   9131.0   9131.0   7025.0   7025.0     NaN   \n",
       "7      6258.0   6258.0  ...  10745.0  10745.0   5811.0   5811.0  7032.0   \n",
       "8      4845.0   4845.0  ...  11855.0  11855.0   6385.0   6385.0  6019.0   \n",
       "9      5166.0   5166.0  ...  13626.0  13626.0   5738.0   5738.0  5986.0   \n",
       "10     4445.0   4445.0  ...  10879.0  10879.0   7081.0   7081.0  6479.0   \n",
       "11     4612.0   4612.0  ...  12276.0  12276.0   6680.0   6680.0  6127.0   \n",
       "12     4366.0   4366.0  ...  11735.0  11735.0   9030.0   9030.0  6690.0   \n",
       "13     4882.0   4882.0  ...  10798.0  10798.0   7646.0   7646.0  6667.0   \n",
       "14     5581.0   5581.0  ...  13508.0  13508.0  10522.0  10522.0  6667.0   \n",
       "15     4994.0   4994.0  ...  12172.0  12172.0  10076.0  10076.0  7500.0   \n",
       "16     5288.0   5288.0  ...  12054.0  12054.0  10059.0  10059.0  7059.0   \n",
       "17     5764.0   5764.0  ...  12101.0  12101.0   9996.0   9996.0  7000.0   \n",
       "18     5801.0   5801.0  ...  16137.0  16137.0  11663.0  11663.0  7000.0   \n",
       "19     5698.0   5698.0  ...  14910.0  14910.0  10202.0  10202.0  6522.0   \n",
       "20     5766.0   5766.0  ...  15617.0  15617.0  10031.0  10031.0  6957.0   \n",
       "21     5683.0   5683.0  ...  13090.0  13090.0  14295.0  14295.0  6957.0   \n",
       "22     5149.0   5149.0  ...  16173.0  16173.0  13009.0  13009.0  7083.0   \n",
       "23     5899.0   5899.0  ...  10419.0  10419.0  12707.0  12707.0  7417.0   \n",
       "24     7088.0   7088.0  ...  14695.0  14695.0  13668.0  13668.0  7200.0   \n",
       "25     6995.0   6995.0  ...  11813.0  11813.0  11266.0  11266.0  7308.0   \n",
       "26     6417.0   6417.0  ...  14275.0  14275.0  12743.0  12743.0  7692.0   \n",
       "27     7269.0   7269.0  ...  14383.0  14383.0  12201.0  12201.0  7407.0   \n",
       "28     6729.0   6729.0  ...  15158.0  15158.0  14892.0  14892.0  7500.0   \n",
       "29     6002.0   6002.0  ...  14976.0  14976.0  15064.0  15064.0  7586.0   \n",
       "30     8766.0   8766.0  ...  14341.0  14341.0  12340.0  12340.0  7667.0   \n",
       "31     8715.0   8715.0  ...  15301.0  15301.0  10875.0  10875.0  7760.0   \n",
       "32     9362.0   9362.0  ...  15410.0  15410.0  12274.0  12274.0  7742.0   \n",
       "33     7879.0   7879.0  ...  14950.0  14950.0  11517.0  11517.0  7758.0   \n",
       "34     8511.0   8511.0  ...  15712.0  15712.0  17028.0  17028.0  7813.0   \n",
       "35     9165.0   9165.0  ...  12049.0  12049.0  15881.0  15881.0  7755.0   \n",
       "36     7045.0   7045.0  ...  12181.0  12181.0  11012.0  11012.0  7879.0   \n",
       "37     8890.0   8890.0  ...  15264.0  15264.0  11987.0  11987.0  7756.0   \n",
       "38     9129.0   9129.0  ...  16339.0  16339.0  17664.0  17664.0  7941.0   \n",
       "39     8590.0   8590.0  ...  15394.0  15394.0  14406.0  14406.0  7692.0   \n",
       "40     9678.0   9678.0  ...  16411.0  16411.0  20435.0  20435.0  7540.0   \n",
       "41     9427.0   9427.0  ...  16388.0  16388.0  18995.0  18995.0  7478.0   \n",
       "42     9963.0   9963.0  ...  16776.0  16776.0   8581.0   8581.0  7420.0   \n",
       "43     9407.0   9407.0  ...  14680.0  14680.0  13706.0  13706.0  7600.0   \n",
       "44    11272.0  11272.0  ...  15738.0  15738.0  11016.0  11016.0  7345.0   \n",
       "45    12039.0  12039.0  ...  15229.0  15229.0  13267.0  13267.0  7274.0   \n",
       "46     9361.0   9361.0  ...  15260.0  15260.0  14273.0  14273.0  7692.0   \n",
       "47    10399.0  10399.0  ...  20558.0  20558.0  13339.0  13339.0  6905.0   \n",
       "48    10020.0  10020.0  ...  20383.0  20383.0  11104.0  11104.0  6526.0   \n",
       "49    10627.0  10627.0  ...  19784.0  19784.0  16479.0  16479.0  5714.0   \n",
       "50     9951.0   9951.0  ...  20776.0  20776.0  13904.0  13904.0  5917.0   \n",
       "51    12030.0  12030.0  ...  20286.0  20286.0  13148.0  13148.0  5441.0   \n",
       "52    11565.0  11565.0  ...  19983.0  19983.0  14180.0  14180.0  5429.0   \n",
       "53    12258.0  12258.0  ...  20193.0  20193.0  16780.0  16780.0  5429.0   \n",
       "last      NaN      NaN  ...      NaN      NaN      NaN      NaN     NaN   \n",
       "\n",
       "         107      108      109      110      111  \n",
       "0        NaN   8222.0   8222.0   9197.0   9197.0  \n",
       "1        NaN   8014.0   8014.0   9059.0   9059.0  \n",
       "2        NaN   7069.0   7069.0   8225.0   8225.0  \n",
       "3        NaN   7889.0   7889.0   8205.0   8205.0  \n",
       "4        NaN   8235.0   8235.0   9308.0   9308.0  \n",
       "5        NaN   8780.0   8780.0   9378.0   9378.0  \n",
       "6        NaN   8696.0   8696.0  12945.0  12945.0  \n",
       "7     7032.0   7852.0   7852.0  10202.0  10202.0  \n",
       "8     6019.0   7740.0   7740.0  11453.0  11453.0  \n",
       "9     5986.0   6244.0   6244.0   9423.0   9423.0  \n",
       "10    6479.0   8763.0   8763.0  13919.0  13919.0  \n",
       "11    6127.0  10071.0  10071.0  16370.0  16370.0  \n",
       "12    6690.0   8387.0   8387.0   9613.0   9613.0  \n",
       "13    6667.0   9677.0   9677.0  14855.0  14855.0  \n",
       "14    6667.0  13111.0  13111.0  13602.0  13602.0  \n",
       "15    7500.0  13757.0  13757.0  13941.0  13941.0  \n",
       "16    7059.0  15047.0  15047.0  14759.0  14759.0  \n",
       "17    7000.0  15154.0  15154.0  13789.0  13789.0  \n",
       "18    7000.0  15291.0  15291.0  11442.0  11442.0  \n",
       "19    6522.0  15675.0  15675.0  11861.0  11861.0  \n",
       "20    6957.0  19306.0  19306.0  17496.0  17496.0  \n",
       "21    6957.0  15721.0  15721.0  11396.0  11396.0  \n",
       "22    7083.0  16512.0  16512.0   6336.0   6336.0  \n",
       "23    7417.0  16461.0  16461.0   7904.0   7904.0  \n",
       "24    7200.0  18428.0  18428.0  18688.0  18688.0  \n",
       "25    7308.0  19306.0  19306.0  16943.0  16943.0  \n",
       "26    7692.0  16103.0  16103.0   8538.0   8538.0  \n",
       "27    7407.0  24574.0  24574.0  15914.0  15914.0  \n",
       "28    7500.0  17173.0  17173.0  15096.0  15096.0  \n",
       "29    7586.0  13520.0  13520.0  16254.0  16254.0  \n",
       "30    7667.0  16403.0  16403.0  13372.0  13372.0  \n",
       "31    7760.0   7631.0   7631.0   4126.0   4126.0  \n",
       "32    7742.0  23029.0  23029.0  14785.0  14785.0  \n",
       "33    7758.0  14175.0  14175.0  14725.0  14725.0  \n",
       "34    7813.0  13314.0  13314.0   5374.0   5374.0  \n",
       "35    7755.0  19117.0  19117.0  15237.0  15237.0  \n",
       "36    7879.0  14134.0  14134.0  12679.0  12679.0  \n",
       "37    7756.0  12112.0  12112.0  11103.0  11103.0  \n",
       "38    7941.0  13236.0  13236.0  10634.0  10634.0  \n",
       "39    7692.0  16823.0  16823.0  14045.0  14045.0  \n",
       "40    7540.0  14021.0  14021.0  11596.0  11596.0  \n",
       "41    7478.0  14192.0  14192.0   5470.0   5470.0  \n",
       "42    7420.0  17019.0  17019.0   8033.0   8033.0  \n",
       "43    7600.0  18144.0  18144.0  10753.0  10753.0  \n",
       "44    7345.0  18987.0  18987.0   5883.0   5883.0  \n",
       "45    7274.0  18160.0  18160.0   8469.0   8469.0  \n",
       "46    7692.0  22526.0  22526.0   7448.0   7448.0  \n",
       "47    6905.0  21798.0  21798.0   3100.0   3100.0  \n",
       "48    6526.0  20664.0  20664.0   4520.0   4520.0  \n",
       "49    5714.0  25335.0  25335.0   7346.0   7346.0  \n",
       "50    5917.0  27314.0  27314.0   5932.0   5932.0  \n",
       "51    5441.0  26893.0  26893.0   6889.0   6889.0  \n",
       "52    5429.0  25324.0  25324.0   6694.0   6694.0  \n",
       "53    5429.0  27554.0  27554.0   5408.0   5408.0  \n",
       "last     NaN      NaN      NaN      NaN      NaN  \n",
       "\n",
       "[55 rows x 112 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crp=pd.read_csv(\"C:/Users/dylan/Downloads/Production_Crops_E_Africa.csv\",encoding = \"ISO-8859-1\")\n",
    "#df_crp=df_crp.drop(['Item Code'], axis=1)\n",
    "df_crp=df_crp.drop(['Unit'], axis=1)\n",
    "df_crp=df_crp.drop(['Element Code'], axis=1)\n",
    "df_crp=df_crp.drop(['Area'], axis=1)\n",
    "df_crp=df_crp.drop(['Area Code'], axis=1)\n",
    "df_crp=df_crp.drop(['Item Code'], axis=1)\n",
    "dr_crp=df_crp.transpose()\n",
    "df_crp=df_crp[df_crp['Element'].str.contains(\"Yield\")]\n",
    "#df_crp=df_crp[df_crp['Element'].str.contains(\"Production\")]\n",
    "df_crp=df_crp[df_crp['Item'].str.contains(\"Cereals,Total\")]\n",
    "df_crp=df_crp.drop(['Element'], axis=1)\n",
    "df_crp=df_crp.drop(['Item'], axis=1)\n",
    "for i in range (0,54):\n",
    "    df_crp = df_crp.drop([df_crp.columns[i+1]],  axis='columns')\n",
    "\n",
    "df_crp = pd.DataFrame(np.repeat(df_crp.values,2,axis=0))\n",
    "df_crp.columns = df_crp.columns\n",
    "df_crp = df_crp.reindex(df_crp.columns.tolist() + ['last'], axis=1)  # version > 0.20.0\n",
    "df_crp=df_crp.transpose()\n",
    "df_crp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.36 1.81 1.63 1.69 1.96 1.21 1.14 0.86 1.23 1.22 1.56 1.46 1.33 1.6\n",
      " 1.16 1.26 1.42 1.38 1.8  2.17 1.22 1.09 0.76 1.04 0.88 2.04 1.43 2.45\n",
      " 1.32 1.68 0.96 0.95 1.54 0.97 1.62 1.42 1.14 1.58 1.31 1.37 0.8  2.09\n",
      " 1.62 1.7  1.38 1.11 0.86 1.25 1.   2.1  1.27 1.6 ]\n",
      "[0.47718878491076777, 0.5453006240635636, 0.4007963645975605, 0.40420601566361225, 0.686151685958797, 0.5605628716925573, 0.3335775292953972, 0.4758086880506992, 0.45583787466382464, 0.6173092072918472, 0.5245991711625351, 0.48165380416393083, 0.4704506649469036, 0.4443911889420794, 0.7407872851838646, 0.6796571124996509, 0.6169032964506507, 0.4660668278619799, 0.6173092072918472, 0.5582897709818561, 0.9048564471955457, 0.7657913930015775, 0.602128141831093, 0.6083791687855212, 0.6734060855452226, 1.086055046705725, 0.6330585479302768, 0.6870446898094297, 0.8685680179925662, 0.7170820920579809, 0.8986054202411174, 0.8593944329815223, 1.193621419622834, 1.0911695233048027, 1.218381980935829, 1.2206550816465302, 1.0176184788799718, 0.8394236195946476, 1.3426718805102396, 1.1374433592012194, 1.1709715946840618, 1.3619120543829601, 1.4719138923472492, 1.111546247532874, 0.6540847295042627, 0.6334644587714735, 0.620231765348463, 0.5785041308734487, 0.6282688000041565, 0.6295677146959857, 0.4896096566513849, 0.5415662443245546, 0.4133796006746563, 0.5016246175508055, 0.3662127609276069, 0.3658068500864103, 0.39340878728778167, 0.37830890399526673, 0.36458911756282036, 0.3083298749729663, 0.3094664253283169, 0.26984952722752503, 0.22519933469589484, 0.26059476004824167, 0.3389355523991928, 0.32237439007837, 0.21756821088139805, 0.24224759002615365, 0.3259464054809004, 0.5302007407710487, 0.4605464404217056, 0.5695740923671226, 0.5036541717567886, 0.4581921575427651, 0.47524041287302393, 0.5091745591970629, 0.5245179889942958, 0.3991727212327739, 0.4736167695082374, 0.3619912881791619, 0.37692880713519816, 0.5298760120980914, 0.4638749093195181, 0.5108793847300888, 0.5377506824173062, 0.4481255686810885, 0.6611475781410842, 0.7211412004699473, 0.4324574102108983, 0.45121049107418293, 0.5582897709818561, 0.5746885689662002, 0.6001797697933492, 0.588245991062168, 0.6121135485245303, 0.601641048821657, 0.5852422508373128, 0.5831315144630903, 0.530769015948724, 0.5272781827144329, 0.4870118272677264, 0.648726706400467, 0.6947569957921659, 0.6788452908172576, 0.5886519019033646, 0.6776275582936677, 0.6930521702591399, 0.688343604501259, 0.7132665301507325, 0.7415179246980186, 0.7462264904558995, 0.7768321678821261, 0.8820442579202946, 0.8196151705442517, 0.9068048192332896, 0.8572025144390604, 0.9438238879504229, 0.8947086761656298, 0.8680809249831303, 0.7672526720298853, 0.9325395665651565, 0.9308347410321306, 0.9225541598717192, 0.9138676678701111, 0.8228624572738248, 1.0131534596268088, 1.0319877226583327, 0.9746731118813675, 1.2321829495365146, 1.1147935342624469, 1.1359820801729115, 1.1855032027989014, 0.36369611371218774, 0.7700128657500225, 0.22373805566758695, 0.5609687825337539, 0.36223483468387985, 0.5221637061153553, 0.336987180361449, 0.21813648605907332, 0.10740400858063046, 0.19256410306368513, 0.22341332699462962, 0.23372346236102423, 0.21537629233893618, 0.12234152753666673, 0.16293261165633055, 0.16114660395506533, 0.15481439483239778, 0.3049202239069146, 0.2345352840434175, 0.21553865667541486, 0.3094664253283169, 0.23437291970693885, 0.34429357550298845, 0.1816045103513759, 0.37205787704083854, 0.26246194991774624, 0.22463105951821954, 0.20027640904642124, 0.18598834743629958, 0.12412753523793194, 0.45007394071883233, 0.2903886157920749, 0.9860386154348734, 0.22325096265815098, 0.3564709007388876, 0.3026471231962134, 0.5189164193857821, 0.2935547203534087, 0.2955030923911526, 0.3017541193455808, 0.36718694694647885, 0.29810092177481107, 0.17665239808877692, 0.2471185201205133, 0.3608547378238113, 0.374412159919779, 0.35444134653290443, 0.39633134534439746, 0.4530776809436875, 0.40542374818720217, 0.42929130564956447, 0.46793401773148446, 0.4709377579563396, 0.46257599462768884, 0.4680963820679631, 0.4613582621040989, 0.41800698426429794, 0.4788936104437937, 0.5754192084803542, 0.5678692668340968, 0.5209459735917654, 0.5901131809316725, 0.5462748100824356, 0.48725537377244443, 0.711642886785946, 0.7075025962057403, 0.7600274590565852, 0.6396343035576624, 0.6909414338849175, 0.7440345719134377, 0.5719283752460631, 0.7217094756476226, 0.7411120138568219, 0.6973548251758243, 0.7856810242202128, 0.7653042999921416, 0.8088179421684212, 0.763680656627355, 0.915085400393701, 0.9773521234332653, 0.7599462768883459, 0.844213367520768, 0.8134453257580628, 0.8627229018793346, 0.8078437561495492, 0.9766214839191113, 0.938871775687824, 0.995131018277678, 0.901690342634212, 0.8319548601166296, 0.9011220674565367, 0.7901460434733758, 0.8968194125398523, 0.8969005947080916, 0.9012032496247759, 1.0019503204097815, 0.8930038506326039, 0.8633723592252494, 0.8770921456576957, 0.8730330372457293, 0.8881329205382442, 0.900229063605904, 0.9356244889582509, 0.9851456115842407, 0.9970793903154219, 0.9695586352822898, 1.1113838831963951, 1.094741538707333, 1.111302701028156, 1.1295686888820047, 1.1309487857420732, 1.0343420055372732, 1.0812652987796048, 1.1252660339653202, 1.1639899282154795, 1.0358032845655811, 1.0630804930939952, 1.001057316559149, 1.04197312935177, 1.06251221791632, 1.0457075090907793, 1.07818037638651, 1.0913318876412812, 1.0366962884162139, 1.0547999119335838, 1.0397812108093083, 1.0520397182134467, 0.9925331888940195, 0.9449604383057736, 0.8984430559046388, 0.9564883061957581, 1.0812652987796048, 0.24622551626988068, 0.15562621651479105, 0.19321356040959975, 0.2551555547762067, 0.36077355565557195, 0.36897295464774404, 0.3044331308974786, 0.27058016674167895, 0.3479467730737582, 0.4313208598555477, 0.3479467730737582, 0.35720154025304157, 0.3131196228990867, 0.40071518242932114, 0.3254593124714645, 0.43375632490272753, 0.49293812554919736, 0.39121686874531986, 0.22227677663927903, 0.2716355349287902, 0.20530970347725958, 0.24525133025100876, 0.2915251661474255, 0.20319896710303706, 0.21342792030119234, 0.25750983765514723, 0.11925660514357228, 0.11487276805864859, 0.943336794940987, 0.6452358731661759, 0.5173739581892349, 0.12055551983540153, 0.3078427819635304, 0.31271371205789, 0.19743503315804478, 0.11422331071273396, 0.08938156723149972, 0.2735027247982948, 0.18736844429636815, 0.17860077012652079, 0.14434189512952453, 0.1592794140855608, 0.14799509270029426, 0.16382561550696315, 0.6533540899901087, 0.6658561438989652, 0.625670970620498, 0.7159455417026304, 0.8585826112991289, 0.7029563947843379, 0.7003585654006794, 0.6792512016584542, 0.663826589692982, 0.7016574800925087, 0.7022257552701839, 0.7227648438347338, 0.7271486809196576, 0.8740072232646012, 0.9369234036500801, 1.1215316542263112, 0.9969170259789433, 0.9465028995023208, 0.9079413695886401, 1.0028433242604142, 0.95746249221463, 0.8704352078620708, 0.77878053991987, 0.786492845902606, 0.9515361939331591, 1.0795604732465787, 1.136631537518826, 1.1426390179685364, 1.5369408091069505, 1.4316475369005426, 1.3873220730418698, 1.3660523449631659, 1.3150699433088682, 1.2690396539171696, 1.355417480923814, 1.3803404065732876, 1.360856686195849, 1.3626426938971141, 1.3986875765953757, 1.3334171133309562, 1.3905693597714428, 1.2920142075288992, 1.360613139691131, 1.3646722481030973, 0.582806785790133, 0.5651902552821989, 0.554311844738129, 0.40802157757086066, 0.4331068675568129, 0.42701820493886333, 0.42417682905048687, 0.4833586296969567, 0.403637740485937, 0.42295909652689695, 0.46241363029121013, 0.4540518669625594, 0.5476549069425042, 0.6268075209758486, 0.8742507697693193, 0.9654995268703235, 0.778049900405716, 0.9029892573260412, 0.8756308666293878, 0.6558707372055279, 0.7253626732183923, 0.8167737946558753, 0.8037034655693435, 0.711642886785946, 0.7058789528409537, 0.7167573633850236, 0.7695257727405865, 0.8002126323350525, 0.8174232520017899, 0.8064636592894806, 0.8383682514075363, 0.8510326696528715, 0.8196151705442517, 0.8269215656857912, 0.8306559454248003, 0.8115781358885583, 0.7722047842924844, 0.7686327688899539, 0.7699316835817832, 1.175030703096028, 1.2308840348446854, 1.3609378683640883, 1.2843830837144024, 1.2010901791008521, 0.5154255861514911, 0.3917851439229951, 0.41622097656303275, 0.5241120781530991, 0.4698823897692283, 0.41467851536648553, 0.42482628639640146, 0.41841289510549456, 0.3991727212327739, 0.4437417315961648, 0.5860540725197061, 0.5486290929613761, 0.4242580112187262, 0.40899576358973255, 0.506739094149883, 0.47410386251767334, 0.449992758550593, 0.5962018435496221, 0.4458524679703873, 0.45397068479432007, 0.5021928927284807, 0.5925486459788524, 0.4067226628790314, 0.6060248859065808, 0.47532159504126326, 0.488148377623077, 0.4724802191528868, 0.5638913405903697, 0.5378318645855455, 0.4290477591448465, 0.5152632218150125, 0.544651166717649, 0.6417450399318849, 0.544894713222367, 0.6183645754789585, 0.6084603509537606, 0.6488890707369457, 0.6590368417668616, 0.48571291257589716, 0.6192575793295911, 0.5130713032725506, 0.7562118971493368, 0.6679668802731877, 0.6946758136239265, 0.7590532730377133, 0.8183162558524225, 0.8757120487976271, 0.9171961367679236, 0.9766214839191113, 0.9393588686972599, 0.9786510381250946, 0.9427685197633118, 0.8595567973180009, 0.8589885221403256, 0.8587449756356076, 0.8589885221403256, 0.9132993926924358, 0.9011220674565367, 0.8839114477897992, 0.9074542765792042, 0.9380599540054307, 0.9395212330337386, 1.040187121650505, 1.0581283808313964, 1.0571541948125243, 1.0489547958203522, 1.0479806098014803, 1.0497666175027456, 1.0513090786992927, 1.0515526252040108, 1.0497666175027456, 1.0385634782857183, 1.0556117336159772, 1.0681137875248337, 1.0630804930939952, 1.0618627605704054, 1.063486403935192, 1.0646229542905425, 1.0644605899540638, 1.037995203108043, 1.0851620428550923, 1.0453015982495826, 1.0738777214698259, 1.1059446779243602, 1.1000995618111287, 1.0712798920861673, 1.1231552975910977, 1.1118709762058312, 0.7731789703113563, 0.7260121305643069, 0.5515516510179919, 0.5733084721061317, 0.525816903686125, 0.5262228145273217, 0.5347469421924511, 0.3714896018631632, 0.5599134143466427, 0.6863140502952757, 0.7940427875488635, 0.5250050820037317, 0.6057813394018627, 0.5503339184944019, 0.6233166877415575, 0.6937828097732939, 0.6253462419475407, 0.6509998071111682, 0.6777899226301464, 0.5068202763181223, 0.5156691326562091, 0.5189164193857821, 0.5248427176672531, 0.53125610895816, 0.597176029568494, 0.6058625215701021, 0.6058625215701021, 0.6135748275528382, 0.5978254869144086, 0.612032366356291, 0.6291618038547891, 0.6267263388076093, 0.6340327339491487, 0.6307042650513364, 0.6255897884522587, 0.6314349045654902, 0.6203129475167024, 0.6262392457981734, 0.6424756794460388, 0.6369552920057645, 0.6449111444932186, 0.6575755627385538, 0.6747861824052912, 0.668778701955581, 0.8120652288979943, 0.668778701955581, 0.682417306219788, 0.7185433710862889, 0.6660996904036832, 0.6082979866172818, 0.6118700020198123, 0.611545273346855, 0.6190952149931125, 0.7528834282515244, 0.7288535064526834, 0.7704187765912192, 0.6370364741740039, 0.7881164892673926, 0.8010244540174457, 0.6912661625578748, 0.688100057996541, 0.7185433710862889, 0.716594999048545, 0.9031516216625198, 0.8546858672236413, 0.8530622238588547, 1.0029245064286534, 0.9181703227867954, 0.9268568147884035, 1.1274579525077821, 1.0445709587354286, 1.2165959732345637, 1.3360149427146149, 1.3657276162902086, 1.3962521115481958, 1.4214185837023874, 1.4831982137325157, 1.505036216988895, 1.4907481553787731, 1.4839288532466697, 1.2740729483480078, 1.4085106189523342, 1.3894328094160924, 1.8437282228833698, 1.5296344139654112, 1.6892385567239292, 1.7611659577839736, 1.7362430321345, 0.5888954484080826, 0.6060248859065808, 0.601641048821657, 0.6039953317005975, 0.6069990719254527, 0.6054566107289054, 0.6000985876251098, 0.591736824296459, 0.6486455242322278, 0.6484019777275097, 0.6677233337684697, 0.6769781009477531, 0.6766533722747957, 0.6902107943707635, 0.6491326172416637, 0.641257946922449, 0.6671550585907944, 0.6477525203815951, 0.6510809892794076, 0.6493761637463816, 0.6514057179523649, 0.6350069199680207, 0.6418262221001242, 0.6440993228108254, 0.621693044376771, 0.6339515517809095, 0.6508374427746896, 0.658387384420947, 0.6391472105482264, 0.6385789353705511, 0.6385789353705511, 0.6267263388076093, 0.6263204279664126, 0.6264827923028913, 0.6264827923028913, 0.626401610134652, 0.626401610134652, 0.626401610134652, 0.626401610134652, 0.6264827923028913, 0.6260768814616946, 0.6262392457981734, 0.6272946139852845, 0.626888703144088, 1.3324429273120844, 1.3356902140416573, 1.3395057759489057, 1.3440519773703081, 1.3530631980448735, 1.3651593411125333, 1.382694689452228, 1.4002300377919228, 1.4357878274807483, 1.4883126903315933, 1.3917059101267935, 1.3530631980448735, 1.3530631980448735, 1.3452697098938982, 1.2177325235899144, 1.3530631980448735, 1.41143317700895, 1.446341509351861, 1.4763789116004122, 1.502600751941715, 1.5285790457783, 1.6236433647865525, 1.6236433647865525, 1.57322923830993, 3.1754405106813, 3.2077510136405527, 3.312232464164567, 3.117719989063138, 3.2295078347286923, 3.1994704324801413, 3.1854259173747375, 3.2629548880432955, 3.2182235133434256, 3.3240038785592696, 3.3288748086536293, 3.4126548062766155, 3.507962671789586, 3.5293947642047687, 3.6850209807195595, 3.812476984855304, 3.9711069415949503, 4.0824888764193075, 4.333260594110591, 4.629737872520615, 4.55675510327346, 4.78853019359674, 4.8669521681159305, 4.758168062675232, 4.792751666345185, 5.276110296042142, 5.362488123048786, 5.518276703900056, 5.818001269039653, 5.910143029991291, 5.763446851982826, 6.0444995184273775, 6.101164671858428, 6.134286996500074, 6.118618838029884, 6.0888249822860505, 6.02249915083452, 6.049857541531173, 5.811263149075789, 5.280331768790587, 5.882784639294637, 5.901131809316725, 5.914364502739735, 5.870120221049302, 0.2118854591046451, 0.22349450916286895, 0.6155231995905821, 0.6482396133910311, 0.7618946489260898, 0.20474142829958428, 0.40599202336487744, 0.41589624789007545, 0.46744692472204846, 0.4799489786309049, 0.5346657600242117, 0.4869306450994871, 0.9119192958323672, 0.9438238879504229, 1.1052140384102063, 1.2691208360854087, 1.1682114009639246, 1.1741376992453953, 1.3658899806266873, 1.4879067794903966, 1.5924694121826508, 1.6616366195225578, 1.7804061316566941, 1.8878101402373246, 0.6368741098375252, 0.6522987218029975, 0.6716200778439575, 0.6592803882715796, 0.692565077249704, 0.863534723561728, 0.8406413521182375, 0.7284475956114868, 0.9122440245053245, 1.0234635949932034, 0.966636077225674, 0.9457722599881668, 1.0844314033409383, 0.9496690040636546, 0.7148901735155191, 0.7838950165189476, 0.9359492176312082, 0.9856327045936767, 0.9516173761013984, 1.0051164249711153, 1.0614568497292087, 0.9383035005101487, 1.1881822143507992, 1.2267437442644797, 1.2453344607912857, 1.2131051400002728, 1.133627797293971, 1.2069352952140837, 1.2272308372739158, 1.2201679886370942, 1.2618956231121086, 1.2383527943227035, 1.1824182804058068, 1.2177325235899144, 1.2370538796308743, 1.2547515923070478, 1.179739268853909, 1.1964627955112106, 1.2826782581813765, 1.3111731992333804, 1.1847725632847474, 1.2893351959770014, 1.3815581390968776, 1.2584047898778175, 1.2895787424817193, 1.3463250780810094, 1.2644934524957672, 1.2916082966877025, 1.2683901965712547, 1.2856819984062315, 1.2982652344833274, 1.3235128888057583, 1.251585487745714, 1.1768978929655325, 1.2492312048667735, 1.2673348283841435, 1.2752094987033584, 1.2831653511908125, 1.3533067445495914, 1.3076823659990895, 1.291445932351224, 1.2972910484644555, 1.2985899631562847, 1.302649071568251, 1.3030549824094477, 1.302892618072969, 0.9030704394942806, 0.8461617395585118, 0.857364878775539, 0.8467300147361871, 0.8042717407470188, 0.6602545742904515, 0.654165911672502, 0.8878081918652869, 0.9717505538247517, 1.0757449113393305, 1.0792357445736214, 0.9992713088578837, 1.015913653346946, 1.1645582033931547, 1.024843691853272, 1.0360468310702993, 0.9073730944109649, 0.9714258251517943, 0.9034763503354771, 0.81498778695461, 0.8997419705964681, 0.9992713088578837, 0.959167317747656, 0.9378164075007127, 0.834796236005006, 0.8895941995665522, 0.7584038156917987, 0.8344715073320487, 1.0496854353345062, 1.0519585360452073, 1.0422166758564881, 0.7795923616022632, 0.9749166583860854, 0.9504808257460479, 0.8445380961937253, 0.8332537748084587, 0.649700892419339, 0.7933121480347095, 0.8537928633730086, 0.9168714080949663, 0.7050671311585605, 0.7390012774825994, 0.7791864507610665, 0.6061872502430594, 0.6987349220358928, 0.7039305808032098, 0.7011703870830727, 0.7769945322186047, 0.6773028296207104, 0.6630147680105887, 0.6754356397512058, 0.6796571124996509, 0.6853398642764038, 0.5827256036218937, 0.6981666468582176, 0.620231765348463, 0.42003653847028116, 0.6820113953785913, 0.7230895725076911, 0.7994819928208985, 0.7619758310943291, 0.8895941995665522, 0.8222941820961496, 0.8030540082234289, 0.9910719098657117, 0.8459993752220332, 1.0880034187434688, 1.0776932833770743, 1.0992877401287353, 1.1277826811807394, 1.0729847176191933, 1.0828077599761519, 1.0529327220640794, 1.0628369465892773, 0.962901697486665, 1.0951474495485296, 1.133627797293971, 1.1148747164306863, 1.1627721956918895, 1.083376035153827, 1.069169155711945, 1.2973722306326947, 1.3474616284363599, 1.4728880783661211, 1.294206126071361, 1.4353819166395518, 1.371004457225765, 1.3828570537887068, 1.1650452964025908, 1.1707280481793436, 1.1669936684403346, 1.1657759359167448, 1.168617311805121, 1.180957001377499, 1.1669124862720952, 1.1437755683238868, 1.1645582033931547, 1.1629345600283683, 1.1777908968161652, 1.169266769151036, 1.1824994625740461, 1.19654397767945, 1.2189502561135044, 1.2316958565270788, 1.240950623706362, 1.2476887436702262, 1.2118874074766828, 1.1812817300504563, 1.1543292501949995, 1.1305428749008766, 1.1408530102672712, 1.1546539788679568, 1.1810381835457382, 1.1843666524435508, 1.1878574856778419, 1.1919165940898082, 1.1963816133429712, 1.2113191322990076, 1.2039315549892287, 1.2073412060552804, 1.2053928340175366, 1.210669674953093, 1.214160508187384, 1.2193561669547008, 1.2290980271434202, 1.189156400369671, 1.2048245588398614, 0.9619275114677931, 0.9957804756235926, 0.9357868532947295, 0.9663113485527167, 1.0117733627667402, 0.540186147464486, 0.5898696344269545, 0.539455507950332, 0.5019493462237627, 0.6155231995905821, 0.6378482958563971, 0.5694929101988833, 0.506495547645165, 0.5177798690304316, 0.5676257203293787, 0.6455606018391333, 0.5579650423088988, 0.5485479107931368, 0.7188680997592461, 1.009987355065475, 1.1164171776272336, 1.2420871740617128, 1.436437284826663, 1.234537232415455, 1.242655449239388, 1.3106861062239445, 1.1704033195063863, 1.1622039205142143, 1.1323288826021418, 1.138336363051852, 1.109354328990412, 0.9013656139612546, 0.9312406518733272, 0.8418590846418275, 0.8905683855854241, 0.8157996086370033, 0.8664572816183438, 0.8938968544832365, 1.0458698734272578, 1.2454156429595251, 1.3481110857822745, 1.0927931666695891, 1.209939035438939, 1.3205903307491424, 1.3407235084724958, 1.2485005653526196, 1.190861225902697, 1.0826453956396733, 1.0729847176191933, 1.0311759009759396, 1.0444085943989498, 1.0604826637103368, 1.0977452789321882, 1.1339525259669283, 1.2849513588920776, 1.258973065055493, 1.1442626613333229, 1.0413236720058554, 1.0080389830277312, 1.2677407392253401, 1.5933624160332833, 1.3605319575228916, 1.0903577016224093, 1.3263542646941346, 1.5312580573301977, 1.2733423088338538, 1.4171159287857031, 1.398768758763615, 1.2680654678982974, 1.3901634489302463, 1.3485169966234711, 1.1862338423130552, 1.5572363511667826, 1.4232045914036526, 1.1386610917248092, 1.1342772546398856, 1.2907964750053094, 1.1590378159528805, 1.1162548132907548, 1.3310628304520158, 1.2082342099059131, 1.2940437617348823, 1.465906411897539, 1.3363396713875721, 1.33658321789229, 1.4394410250515182, 1.150757234792469, 1.0089319868783637, 1.3882962590607417, 1.2295851201528563, 1.4164664714397883, 1.3488417252964284, 1.3213209702632964, 0.5056025437945324, 0.3045143130657179, 0.4773511492472464, 0.7383518201366848, 0.5707918248907126, 0.5196470588999361]\n",
      "0.0 actual\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMYAAADFCAYAAAAPD43zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEhJJREFUeJztnX+MVNd1x79nh4HMkoTFMmrNAl7bqrDiYlh35R9aKRKkNUmhZEWTOthOpaoK/7SVca2NoE3qdYpqqlVT/9GqErLTpDJxcAxZ4dgNiQRRkqq2vMtC6AaILMvYDLFYy16CYe2d3T39Y2aW2dn73rtv5r1338x8P9Li8eybeefNvu/ce84951xRVRBC5tPm2gBC0giFQYgBCoMQAxQGIQYoDEIMUBiEGKAwCDFAYRBiYJHNQSLyJoArAGYATKtqT5xGEeIaK2GU2Kiq79oceOONN2pXV1dtFhESIyMjI++q6oqg48IIw5quri4MDw/H8daE1IWInLc5ztbHUAA/FpEREdnpccKdIjIsIsPj4+O2dhKSSmyF0auqdwH4HIC/EpFPVx+gqvtVtUdVe1asCBypCEk1VsJQ1Yul/14C8AMAd8dpFCGuCfQxRGQpgDZVvVJ6fD+Ab0RtyNBoHoNHz+HixCRWduTQv3kt+ro7oz4NIVbYON+/A+AHIlI+/ruq+qMojRgazWPP4dOYLMwAAPITk9hz+DQAUBzECYHCUNU3AKyP04jBo+fmRFFmsjCDwaPnKAzihFSsfF+cmAz1PCFxkwphrOzIhXqekLhJhTD6N69FLpuZ91wum0H/5rWOLCKtTiwr32Ep+xGMSpG0kAphAEVxUAgkLaRiKkVI2qAwCDFAYRBigMIgxACFQYgBCoMQAxQGIQYoDEIMUBiEGKAwCDGQmpSQKGAVIImKphEGqwBJlDTNVMqvCpCQsDSNMFgFSKKkaYTBKkASJU0jDFYBkihpGuebVYAkSppGGACrAEl0WE+lRCQjIqMi8sM4DSIkDYTxMR4BcCYuQwhJE1bCEJFVALYAeDpecwhJB7YjxlMAvgpg1usA7o9BmolAYYjIVgCXVHXE7zjuj0GaCZsRoxfAttIGld8DsElEno3VKkIcEygMVd2jqqtUtQvAlwAcU9WHY7eMEIekYh2D6eIkbYQShqr+FMBPozQgKF2coiEucD5iBKWLs8aCuMB5EqFfujhrLIgrnAvDL12cNRbEFc6F4ZcuzhoL4grnPkZQuniljwGwxoIkg3NhAN7p4kGiYcSKxEUqhOGHl2jYFYTEiXMfo1YYsSJx0rDCYMSKxEnDCoMRKxInDSsMdgUhcZJ659sLdgUhcdKwwgDYFYTER8NOpQiJEwqDEAMUBiEGKAxCDFAYhBigMAgxQGEQYoDCIMQAhUGIgcCVbxH5GICfAVhSOv4FVX08bsO8YHESSQKblJCPAGxS1Q9EJAvgFyLy36r6Ssy2LYDFSSQpbFp0qqp+UPrfbOlHY7XKAxYnkaSw3R8jIyInAVwC8BNVfdVwTOzbALA4iSSFlTBUdUZVNwBYBeBuEfl9wzGxbwPA4iSSFKGiUqo6gWLv2s/GYk0ANsVJQ6N59O47hlt2v4TefccwNJpP2kzSBNhEpVYAKKjqhIjkAPwhgH+O3TIDNu10wjjnjHARL0TV348WkTsBfAdABsUR5nlV/Ybfa3p6enR4eDgyI72ovrGvTU3j/WuFBcd1duTwP7s3LXitqZnbk9vXURxNjIiMqGpP0HGBI4aq/hJAdyRWRYhpdPDC5Jz7RbgoDNKwK9+mG9sLk3POCBfxo2GFYXsDe3UOYYSL+NGwwvC6gTtyWXR25CAo+hZePgPb7xA/GrZLSP/mtUbneWDbHVY+AtvvED8aVhg2N3ZQOJbtd4gXDSsMwP/GZsIhqYeGEEYtC3EMx5J6SL3zPTSaR/8Lp5CfmISi+M2/6+BJfG3otO/rGI4l9ZB6YTzx4hgKMwtX55995S3fPCiGY0k9pE4Y1UmAphSPMn51GAzHknpIlY8RJs0D8J8WMRxL6iFVwhg4Mmad5gEAbSIYGs173uyVUauyA//owZMUCQkkNcIYGs1jYtJ72mRiRtUqBMvQLQlLanyMWuu2bWq+WStOwpIaYdQTRg16LUO3JCypmUqt7MgFOtt+ry1jWgz0em+GbokXqRkxTOFVGypDsGVfonIxcM/h09h4+wqGbkkoUiMMAFiyKJw51WnlXr7E8bPjeHL7OrRnr7//h9MzGD7/Xv1Gk6YkFVMpU/11EB257Lw67qHRvOdU7OLEJIbPv4drhdm551SLq+cAsLdvXY2Wk2YlFSNGmDLVMpcrQrtlYXmxLJedE0E1z736dqjzktYgFcKoJTrU0Z6de+wnrGyb4MpH057vMxPQJYW0JqkQhld0KCPi+ZoPPpyeSyL0E9biRW2YmfW++f3OQVqXVAjDK+Fvxz2rPSNVhVmdW6DzElZnRw5Xp/ynaPfeurwGi0mzEygMEVktIsdF5IyIjInII1Eb0dfdiSe3r1vQxGBv3zo8ud3bMS6PFBtvN/fK9Xq+khNvXbZq48nWn62FTVRqGsBjqnpCRD4BYEREfqKqv4rSEK8y1b7uTgwePee7QHf8rLm7+vGz4+jIZX1zsGyq+phr1XrY7I/xG1U9UXp8BcAZAIneDUG1FX4pH1vX3xT4/n4+ytBoHo89f4q5Vi1GKB9DRLpQbNeZ6P4YXlOt8re1X7We12hSfZyJ8kjhFblirlXzYr3AJyIfB3AIwC5V/W3171V1P4D9QLGpc2QWlvDrCNK/eS36v38KhYroU7ZN0L95LR49eNL3fQXevkjQ+gpzrZoXK2GU9t47BOCAqh6O1yRvqhMEN96+AsfPjptXvEtR2KDkRAVwaCSPnptvWCA8vxGBuVbNjU1USgA8A+CMqn4zfpPMDI3m0f/9+d1Cnn3lLc+bvjBTDOfaJCd6+Qt+6yvcLqC5sfExegF8GcAmETlZ+vnjmO1awMCRsXlTJRvyE5PW6Sam0cHL6f+XP1tPUTQ5Nvtj/AJzE5PkKU+fwpa9AkWjbWs8TKMDGyq0LqnIrvXia0OnceCVt2reO9n2dX7+AvvbtiapFcbQaL4uUdiyvD2Lx//kDgBA775jHBkIgBQL44kXx2IXBQBMXCtgV1VIlyvbJFXCGBrNY+DIWE3+RK14iW+yMIOBI2MURouSiuxa4Ho4NklRBDExWWCyYIuSGmEMHj0XOhybBMyHak1SI4y05h2l1S4SL6kQxtBoHm0prqTjdKr1cC6MoAxW1yiAPYdPUxwthnNh1NIhJGlYe9F6OA/XNsocvlHsTAK/LOeMCGZU0dngi6TOhVFPz9okYe1FURBPvDg2b5ercpZzmfKU2G+RtJbNRpNGNIa5fU9Pjw4PD1sdOzSax6MHTyayyl0r2Yxg6eJFuDxZcPaHTPpmMo0Kh0byoae9nR25BR0jTV0nl7dnseXOm3D87LjxGqO6fhEZUdWeoOOcjxgA0L44E9jmxhXL27P44MPpuYVHF+kiSTdjMJ2v1ry16imol0/5/rXCvJGn8hoBJN6MwqkwaulZmxTZjGDwC+sxePTcgg0yk94vPOk9y03nq3VEr56ChvHVytd49aNp4/U/9vwpDJ9/b0EVZ0YEO+5ZXVdPYqfCSHNEauniRejr7vSsGU/SGfc6V35iEr37joWeVlROS5blshApJlOWpyhRXZspnT+sT+l37IyqsSdx5fO1isNpuDbNkZ6JyQJu2/MyPpY1f0RJOuN+5ypPK2zXWar3EJmYLOD9a4V5+4lU9gWuJOwSrKn8N+w+KPW0UK2nYbfTEcNFROqNxQ8i9Ge9xPDchwAG6jBk4LL1oaYuKJUETasqR4i2UjjVi8nCDJYsakMum5k3mueyGfzpH3TOc4433r7C0/fo7Mh5NtADYJVFXW1DWOpZNHYqjP7Na2vyMV5f/CAydWSQpCL7ZGCZlTjKN3VQgmX16Ft+XX5iEoLrPoLNzXJ5soB/fWCDdRSoWhxBHVTKVZFe6yGV5/TqQmlDPaONU2GUP+jHnj+14A8WdPOn4uaOmTDBiWW57FwF4rJcFlenplGYKX6mYb83V5a+7W38lr1969Bz8w01hVJtz1FrgGbHPatDv6aM83Bt34/uw+eXXDb+9Vrh5vfDNjjRBuDq1PWQcj01LbX0y4qzLr66IUVHexaqxWusHAmrWbKorQGjUv/4u8DM9eFR5v4hldgGJ2YBzM6EGxcyIphVNUal0rYKbRJe775jvlOsqelZz9/ZkLwwqkRBvIkrOJHLZhq2YVyl7+RHvVFDm06E3xKRSyLyf3WdqUzKRKEa8ieqE1s43rVu8WyiPCBXN8RuFIZG89jwxI+x6+BJqy+Letun2owY3wbwbwD+q64zxUg96V4/n70Df174e+PvRICVy3LOkt3K59pz+JeYLNQ2NRAgtVMkW8JmSCxvz9Z9rTadCH9Wav/vFL+b/33N4a6pZyI/50P3rHG+1XF5fn3L7pdCj1bVCXyNSpgMiVw2M9cnrB4i8zFEZCeAnQCwZs0a7wMzOd/plJcA4rr5vWjP1hfViJqH7l3juSWziTR3Yw+bKWsbhIiyBiQyYVjvj/H1d+Y54Dr3T5FZBW6b+m5UZtXMP22/07UJ8yiL9LlX316w5tMmxc+tEYqEaskUtglCCBDp6OgmXPv1d+Ye3lrDFCEJ0nhT7e1bl6pRrBZqyRQubwDkd59EnbvmvOY7jZVxnSm0qVnw2y/Ri77uTjx0r/f0PJuRyKeNNuHa5wD8L4C1InJBRP4ySgNsthxOkjTPzZsBv/0S/djbtw5PPbABy6syf5e3ZzH4hej3K7GJSu2I9IwVDI3m60oNjppy5/M0TqOaBVPiqO2XUZJbMjjLlUpbP6nl7VmM/sP9rs1oehplMx5nwkhb9d6WO4P3AyfR0Aib8ThzvtNWvWezHzhpHZwJI23RqEbobUWSw5kw+jevRbaeMjxLmM1OasHtOkYCfreiuC4h4PoEscep853ERjHViXRdu1+K/Zyk8Wlq59sUH++97QbjsV7Pk9bEmTCW5cy9i2xZ3p7Fw/euWVDIE1SQc+Ar9y0QQe9tN+DAV+6ryx7SXDibSk1N17aGUZ05WkuHCoqABOFMGNdCVqRl2wSDX1yYE9MIi0Wk8XCeXWuLSRSExEVDCGPp4gxFQRLFmTA6QjjfU9Oz3BySJIozYWxdb5+0V5hVbg5JEsWZMMIm7aUt6ZA0N86EETZpL21Jh6S5cSaMthDZfSw3JUnjbB3DNk0qza1gSPPifBsAP97ct8W1CaRFaYhwLSFJ40wYA9vuQNbH0aBwiEushCEinxWRcyLyuojsjuLEfd2dGPzieqMAsm2CgW31N+YlpFZsGq5lAPw7gM8B+BSAHSLyqShO3tfdiZOP34+nHtgwr8qOeVHENTbO990AXlfVNwBARL4H4PMAfhWVEcyQJWnDZirVCaCyXeCF0nPzEJGdIjIsIsPj42xFQxobG2GYPOQFqxCqul9Ve1S1Z8WKdPWjJSQsNsK4AKByw+RVAC7GYw4h6UA0oHesiCwC8GsAnwGQB/AagAdVdcznNeMAzgec+0YA74ayNn3wGtJBmGu4WVUDpzQ23c6nReSvARwFkAHwLT9RlF4TeGIRGVbVnqDj0gyvIR3EcQ1WKSGq+jKAl6M8MSFppiFKWwlJGpfC2O/w3FHBa0gHkV9DoPNNSCvCqRQhBigMQgw4EUYc2bpJIiKrReS4iJwRkTERecS1TbUgIhkRGRWRH7q2pRZEpENEXhCRs6W/RWS9VxP3MUrZur8G8Ecorqq/BmCHqkaWlBg3InITgJtU9YSIfALACIC+RroGABCRvwXQA+CTqrrVtT1hEZHvAPi5qj4tIosBtKvqRBTv7WLEmMvWVdUpAOVs3YZBVX+jqidKj68AOANDYmWaEZFVALYAeNq1LbUgIp8E8GkAzwCAqk5FJQrAjTCssnUbBRHpAtAN4FW3loTmKQBfBRCuu3Z6uBXAOID/LE0HnxaRpVG9uQthWGXrNgIi8nEAhwDsUtXfurbHFhHZCuCSqo64tqUOFgG4C8B/qGo3gKsAIvNXXQijKbJ1RSSLoigOqOph1/aEpBfANhF5E8Wp7CYRedatSaG5AOCCqpZH6hdQFEokuBDGawB+T0RuKTlMXwJwxIEdNSMiguLc9oyqftO1PWFR1T2qukpVu1D8/I+p6sOOzQqFqr4D4G0RKXfi+wwirCpNvK9ULdm6KaQXwJcBnBaRk6Xn/q6UbEmS428AHCh9wb4B4C+iemOmhBBigCvfhBigMAgxQGEQYoDCIMQAhUGIAQqDEAMUBiEG/h9085kVIfezAQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23198920725114033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1100: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1100: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1100: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.014174099308391419, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.732861181061203, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.87000931980004, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15.58725722290113, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19.124571801589703, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21.831492054933108, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23.954989983417214, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25.658706464740938, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5739895499181387, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.8278476665364, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.540407938410688, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.025192414262818, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16.61592712906472, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.593000183797702, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.140164695047165, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21.376738364003543, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.38243768167826, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.769495188376041, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.622230316893486, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16.465327432945415, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21.66683273441587, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25.633824252508457, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.72657949016589, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.192869849146472, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33.199094944942395, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.14302001134586817, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.375740708282521, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.459946240817345, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.170481695706528, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17.648353637868333, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.262796789269345, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.277277683088595, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23.86444677516868, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25.138621723275303, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19891524358149582, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9680117322074864, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.319368946563472, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15.245535712661763, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.921045053760253, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21.71673624501342, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23.897371872319482, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25.637340600585063, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27.052700026539814, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 54.20402957360714, tolerance: 0.012167687691980447\n",
      "  positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1100: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01417381408613494, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.363697508756971, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.325235118057819, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6755523911376713, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7954703542834807, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.103675822877136, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12.717793694726538, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.593520352864772, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 30.15162709373255, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.23506538958663725, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5366853263993647, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.073522781048524, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.246105661850777, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.296392895439922, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.150411362032905, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19.220762966930206, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26.27921780488313, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.21090909480045, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 34.502263897493634, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.7615583949949212, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9016711821712846, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.6813640934691705, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.558080865767764, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.8669692679628795, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.166162511716458, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27.570720968404704, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 34.57790801144017, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 39.3852245369776, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1430197769430066, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9335487991523763, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9033302072819112, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.226670971201088, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3233716269137688, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.83255657650622, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19.86117865419596, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.967156100299686, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35.479391859581824, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 39.880010695461365, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.19891489500692217, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2404503738642347, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2891709256514474, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.66098707338935, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.782070655473305, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.973833162420306, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17.860634491394798, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27.343443571353912, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 34.231752212191154, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 38.91014526447941, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52.92441864803846, tolerance: 0.012167687691980447\n",
      "  positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1100: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.042530490093071194, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.1880920886690376, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2455315206653665, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.610989409793397, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.376229376854901, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 12.68559954179193, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.742848217284205, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 30.021144470555733, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 34.45495295815374, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.07142864676828253, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5697097793699726, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8224374300380504, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.7181758960351274, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.602009597426076, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16.934612441644987, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25.259084150692114, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 30.533421463939767, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33.89139552817533, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 36.060600689902415, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8157929687709213, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.072036596102052, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9453139928366596, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.535405365452114, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 18.94482540656601, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26.231184942912208, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 34.12872052499267, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 39.08830335909837, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 42.23477899344215, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1488676183672482, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.197764462636954, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7783015707803997, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.415189318133123, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15.924543474178925, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27.229326484235504, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 34.45475886346888, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 39.04733212452625, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 42.012837522813726, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.01305405162469242, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.72952388138701, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.8189191300109258, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7885189171312703, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.731644641109845, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19.067635297137656, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 25.726392942936148, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33.404813424282494, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 38.268361532527564, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 41.38825088026011, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 52.080756706160614, tolerance: 0.012167687691980447\n",
      "  positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1100: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.022924515699259018, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4645035261548287, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.951279989096626, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 10.609191229750785, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17.736886434021883, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 23.621587495380016, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 30.65414983357838, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35.22107023326211, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5356247010924164, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.4582954352843913, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.627758108019918, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11.432397657852214, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.78368336187763, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26.5916259772004, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 30.58928062787711, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33.368754770462985, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35.328449398866034, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.38066972205091076, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.13336242917174, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.268865712495909, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.961891905212781, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.656665955256237, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.574911220871524, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35.45527674150098, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 39.30366471563231, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 41.940559026769236, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8690258378303497, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.159109417225039, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.044012741707419, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.228704457855386, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21.582353940133473, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.413995515413387, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 34.78530527655008, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 38.514577537752515, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 41.14340386828402, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.48984399774622034, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.380059329024576, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.5293559159074945, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.793061484731389, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.136442976494614, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.67595197741434, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 34.35602140477498, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 38.23440629653189, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 40.8778945002111, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 53.07234881088192, tolerance: 0.012167687691980447\n",
      "  positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:1100: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.015677087984101945, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.865434419588425, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.65594617268566, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 13.653096873721559, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17.135027730372222, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24.308903670716067, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 29.87930672366507, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33.9919590649889, tolerance: 0.009339368242505601\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7562807686281303, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.997414758202638, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.387841292760896, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.925054270700485, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 20.337167771445884, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 24.366001496302903, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.12075436253702, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.146299778457962, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33.53229771044962, tolerance: 0.008826055253583435\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.35904977320547005, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.983708614363465, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.531536502529967, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.602720284720206, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.84967638187188, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 28.61984519770524, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 32.9644194238752, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 36.84454407639133, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 39.76691465672528, tolerance: 0.010087601121241688\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.8994695855280668, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.236078811592556, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.901895085906965, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14.488073822736531, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21.665324828711, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26.88626382357826, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.73705845661361, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35.61739081246369, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 38.66775085283582, tolerance: 0.01017453714603715\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.5378400553146037, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.836708458660283, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.9982827507543774, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 15.07284939988839, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 22.433037510047278, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27.633274378051382, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 31.638166511660522, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35.3917038602389, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:471: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 38.37870566666359, tolerance: 0.010239051614027935\n",
      "  tol, rng, random, positive)\n",
      "C:\\Users\\dylan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 53.424807557363124, tolerance: 0.012167687691980447\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1873868872570976]\n",
      "1000 1000 [0.47718878491076777, 0.5453006240635636, 0.4007963645975605, 0.40420601566361225, 0.686151685958797, 0.5605628716925573, 0.3335775292953972, 0.4758086880506992, 0.45583787466382464, 0.6173092072918472, 0.5245991711625351, 0.48165380416393083, 0.4704506649469036, 0.4443911889420794, 0.7407872851838646, 0.6796571124996509, 0.6169032964506507, 0.4660668278619799, 0.6173092072918472, 0.5582897709818561, 0.9048564471955457, 0.7657913930015775, 0.602128141831093, 0.6083791687855212, 0.6734060855452226, 1.086055046705725, 0.6330585479302768, 0.6870446898094297, 0.8685680179925662, 0.7170820920579809, 0.8986054202411174, 0.8593944329815223, 1.193621419622834, 1.0911695233048027, 1.218381980935829, 1.2206550816465302, 1.0176184788799718, 0.8394236195946476, 1.3426718805102396, 1.1374433592012194, 1.1709715946840618, 1.3619120543829601, 1.4719138923472492, 1.111546247532874, 0.6540847295042627, 0.6334644587714735, 0.620231765348463, 0.5785041308734487, 0.6282688000041565, 0.6295677146959857, 0.4896096566513849, 0.5415662443245546, 0.4133796006746563, 0.5016246175508055, 0.3662127609276069, 0.3658068500864103, 0.39340878728778167, 0.37830890399526673, 0.36458911756282036, 0.3083298749729663, 0.3094664253283169, 0.26984952722752503, 0.22519933469589484, 0.26059476004824167, 0.3389355523991928, 0.32237439007837, 0.21756821088139805, 0.24224759002615365, 0.3259464054809004, 0.5302007407710487, 0.4605464404217056, 0.5695740923671226, 0.5036541717567886, 0.4581921575427651, 0.47524041287302393, 0.5091745591970629, 0.5245179889942958, 0.3991727212327739, 0.4736167695082374, 0.3619912881791619, 0.37692880713519816, 0.5298760120980914, 0.4638749093195181, 0.5108793847300888, 0.5377506824173062, 0.4481255686810885, 0.6611475781410842, 0.7211412004699473, 0.4324574102108983, 0.45121049107418293, 0.5582897709818561, 0.5746885689662002, 0.6001797697933492, 0.588245991062168, 0.6121135485245303, 0.601641048821657, 0.5852422508373128, 0.5831315144630903, 0.530769015948724, 0.5272781827144329, 0.4870118272677264, 0.648726706400467, 0.6947569957921659, 0.6788452908172576, 0.5886519019033646, 0.6776275582936677, 0.6930521702591399, 0.688343604501259, 0.7132665301507325, 0.7415179246980186, 0.7462264904558995, 0.7768321678821261, 0.8820442579202946, 0.8196151705442517, 0.9068048192332896, 0.8572025144390604, 0.9438238879504229, 0.8947086761656298, 0.8680809249831303, 0.7672526720298853, 0.9325395665651565, 0.9308347410321306, 0.9225541598717192, 0.9138676678701111, 0.8228624572738248, 1.0131534596268088, 1.0319877226583327, 0.9746731118813675, 1.2321829495365146, 1.1147935342624469, 1.1359820801729115, 1.1855032027989014, 0.36369611371218774, 0.7700128657500225, 0.22373805566758695, 0.5609687825337539, 0.36223483468387985, 0.5221637061153553, 0.336987180361449, 0.21813648605907332, 0.10740400858063046, 0.19256410306368513, 0.22341332699462962, 0.23372346236102423, 0.21537629233893618, 0.12234152753666673, 0.16293261165633055, 0.16114660395506533, 0.15481439483239778, 0.3049202239069146, 0.2345352840434175, 0.21553865667541486, 0.3094664253283169, 0.23437291970693885, 0.34429357550298845, 0.1816045103513759, 0.37205787704083854, 0.26246194991774624, 0.22463105951821954, 0.20027640904642124, 0.18598834743629958, 0.12412753523793194, 0.45007394071883233, 0.2903886157920749, 0.9860386154348734, 0.22325096265815098, 0.3564709007388876, 0.3026471231962134, 0.5189164193857821, 0.2935547203534087, 0.2955030923911526, 0.3017541193455808, 0.36718694694647885, 0.29810092177481107, 0.17665239808877692, 0.2471185201205133, 0.3608547378238113, 0.374412159919779, 0.35444134653290443, 0.39633134534439746, 0.4530776809436875, 0.40542374818720217, 0.42929130564956447, 0.46793401773148446, 0.4709377579563396, 0.46257599462768884, 0.4680963820679631, 0.4613582621040989, 0.41800698426429794, 0.4788936104437937, 0.5754192084803542, 0.5678692668340968, 0.5209459735917654, 0.5901131809316725, 0.5462748100824356, 0.48725537377244443, 0.711642886785946, 0.7075025962057403, 0.7600274590565852, 0.6396343035576624, 0.6909414338849175, 0.7440345719134377, 0.5719283752460631, 0.7217094756476226, 0.7411120138568219, 0.6973548251758243, 0.7856810242202128, 0.7653042999921416, 0.8088179421684212, 0.763680656627355, 0.915085400393701, 0.9773521234332653, 0.7599462768883459, 0.844213367520768, 0.8134453257580628, 0.8627229018793346, 0.8078437561495492, 0.9766214839191113, 0.938871775687824, 0.995131018277678, 0.901690342634212, 0.8319548601166296, 0.9011220674565367, 0.7901460434733758, 0.8968194125398523, 0.8969005947080916, 0.9012032496247759, 1.0019503204097815, 0.8930038506326039, 0.8633723592252494, 0.8770921456576957, 0.8730330372457293, 0.8881329205382442, 0.900229063605904, 0.9356244889582509, 0.9851456115842407, 0.9970793903154219, 0.9695586352822898, 1.1113838831963951, 1.094741538707333, 1.111302701028156, 1.1295686888820047, 1.1309487857420732, 1.0343420055372732, 1.0812652987796048, 1.1252660339653202, 1.1639899282154795, 1.0358032845655811, 1.0630804930939952, 1.001057316559149, 1.04197312935177, 1.06251221791632, 1.0457075090907793, 1.07818037638651, 1.0913318876412812, 1.0366962884162139, 1.0547999119335838, 1.0397812108093083, 1.0520397182134467, 0.9925331888940195, 0.9449604383057736, 0.8984430559046388, 0.9564883061957581, 1.0812652987796048, 0.24622551626988068, 0.15562621651479105, 0.19321356040959975, 0.2551555547762067, 0.36077355565557195, 0.36897295464774404, 0.3044331308974786, 0.27058016674167895, 0.3479467730737582, 0.4313208598555477, 0.3479467730737582, 0.35720154025304157, 0.3131196228990867, 0.40071518242932114, 0.3254593124714645, 0.43375632490272753, 0.49293812554919736, 0.39121686874531986, 0.22227677663927903, 0.2716355349287902, 0.20530970347725958, 0.24525133025100876, 0.2915251661474255, 0.20319896710303706, 0.21342792030119234, 0.25750983765514723, 0.11925660514357228, 0.11487276805864859, 0.943336794940987, 0.6452358731661759, 0.5173739581892349, 0.12055551983540153, 0.3078427819635304, 0.31271371205789, 0.19743503315804478, 0.11422331071273396, 0.08938156723149972, 0.2735027247982948, 0.18736844429636815, 0.17860077012652079, 0.14434189512952453, 0.1592794140855608, 0.14799509270029426, 0.16382561550696315, 0.6533540899901087, 0.6658561438989652, 0.625670970620498, 0.7159455417026304, 0.8585826112991289, 0.7029563947843379, 0.7003585654006794, 0.6792512016584542, 0.663826589692982, 0.7016574800925087, 0.7022257552701839, 0.7227648438347338, 0.7271486809196576, 0.8740072232646012, 0.9369234036500801, 1.1215316542263112, 0.9969170259789433, 0.9465028995023208, 0.9079413695886401, 1.0028433242604142, 0.95746249221463, 0.8704352078620708, 0.77878053991987, 0.786492845902606, 0.9515361939331591, 1.0795604732465787, 1.136631537518826, 1.1426390179685364, 1.5369408091069505, 1.4316475369005426, 1.3873220730418698, 1.3660523449631659, 1.3150699433088682, 1.2690396539171696, 1.355417480923814, 1.3803404065732876, 1.360856686195849, 1.3626426938971141, 1.3986875765953757, 1.3334171133309562, 1.3905693597714428, 1.2920142075288992, 1.360613139691131, 1.3646722481030973, 0.582806785790133, 0.5651902552821989, 0.554311844738129, 0.40802157757086066, 0.4331068675568129, 0.42701820493886333, 0.42417682905048687, 0.4833586296969567, 0.403637740485937, 0.42295909652689695, 0.46241363029121013, 0.4540518669625594, 0.5476549069425042, 0.6268075209758486, 0.8742507697693193, 0.9654995268703235, 0.778049900405716, 0.9029892573260412, 0.8756308666293878, 0.6558707372055279, 0.7253626732183923, 0.8167737946558753, 0.8037034655693435, 0.711642886785946, 0.7058789528409537, 0.7167573633850236, 0.7695257727405865, 0.8002126323350525, 0.8174232520017899, 0.8064636592894806, 0.8383682514075363, 0.8510326696528715, 0.8196151705442517, 0.8269215656857912, 0.8306559454248003, 0.8115781358885583, 0.7722047842924844, 0.7686327688899539, 0.7699316835817832, 1.175030703096028, 1.2308840348446854, 1.3609378683640883, 1.2843830837144024, 1.2010901791008521, 0.5154255861514911, 0.3917851439229951, 0.41622097656303275, 0.5241120781530991, 0.4698823897692283, 0.41467851536648553, 0.42482628639640146, 0.41841289510549456, 0.3991727212327739, 0.4437417315961648, 0.5860540725197061, 0.5486290929613761, 0.4242580112187262, 0.40899576358973255, 0.506739094149883, 0.47410386251767334, 0.449992758550593, 0.5962018435496221, 0.4458524679703873, 0.45397068479432007, 0.5021928927284807, 0.5925486459788524, 0.4067226628790314, 0.6060248859065808, 0.47532159504126326, 0.488148377623077, 0.4724802191528868, 0.5638913405903697, 0.5378318645855455, 0.4290477591448465, 0.5152632218150125, 0.544651166717649, 0.6417450399318849, 0.544894713222367, 0.6183645754789585, 0.6084603509537606, 0.6488890707369457, 0.6590368417668616, 0.48571291257589716, 0.6192575793295911, 0.5130713032725506, 0.7562118971493368, 0.6679668802731877, 0.6946758136239265, 0.7590532730377133, 0.8183162558524225, 0.8757120487976271, 0.9171961367679236, 0.9766214839191113, 0.9393588686972599, 0.9786510381250946, 0.9427685197633118, 0.8595567973180009, 0.8589885221403256, 0.8587449756356076, 0.8589885221403256, 0.9132993926924358, 0.9011220674565367, 0.8839114477897992, 0.9074542765792042, 0.9380599540054307, 0.9395212330337386, 1.040187121650505, 1.0581283808313964, 1.0571541948125243, 1.0489547958203522, 1.0479806098014803, 1.0497666175027456, 1.0513090786992927, 1.0515526252040108, 1.0497666175027456, 1.0385634782857183, 1.0556117336159772, 1.0681137875248337, 1.0630804930939952, 1.0618627605704054, 1.063486403935192, 1.0646229542905425, 1.0644605899540638, 1.037995203108043, 1.0851620428550923, 1.0453015982495826, 1.0738777214698259, 1.1059446779243602, 1.1000995618111287, 1.0712798920861673, 1.1231552975910977, 1.1118709762058312, 0.7731789703113563, 0.7260121305643069, 0.5515516510179919, 0.5733084721061317, 0.525816903686125, 0.5262228145273217, 0.5347469421924511, 0.3714896018631632, 0.5599134143466427, 0.6863140502952757, 0.7940427875488635, 0.5250050820037317, 0.6057813394018627, 0.5503339184944019, 0.6233166877415575, 0.6937828097732939, 0.6253462419475407, 0.6509998071111682, 0.6777899226301464, 0.5068202763181223, 0.5156691326562091, 0.5189164193857821, 0.5248427176672531, 0.53125610895816, 0.597176029568494, 0.6058625215701021, 0.6058625215701021, 0.6135748275528382, 0.5978254869144086, 0.612032366356291, 0.6291618038547891, 0.6267263388076093, 0.6340327339491487, 0.6307042650513364, 0.6255897884522587, 0.6314349045654902, 0.6203129475167024, 0.6262392457981734, 0.6424756794460388, 0.6369552920057645, 0.6449111444932186, 0.6575755627385538, 0.6747861824052912, 0.668778701955581, 0.8120652288979943, 0.668778701955581, 0.682417306219788, 0.7185433710862889, 0.6660996904036832, 0.6082979866172818, 0.6118700020198123, 0.611545273346855, 0.6190952149931125, 0.7528834282515244, 0.7288535064526834, 0.7704187765912192, 0.6370364741740039, 0.7881164892673926, 0.8010244540174457, 0.6912661625578748, 0.688100057996541, 0.7185433710862889, 0.716594999048545, 0.9031516216625198, 0.8546858672236413, 0.8530622238588547, 1.0029245064286534, 0.9181703227867954, 0.9268568147884035, 1.1274579525077821, 1.0445709587354286, 1.2165959732345637, 1.3360149427146149, 1.3657276162902086, 1.3962521115481958, 1.4214185837023874, 1.4831982137325157, 1.505036216988895, 1.4907481553787731, 1.4839288532466697, 1.2740729483480078, 1.4085106189523342, 1.3894328094160924, 1.8437282228833698, 1.5296344139654112, 1.6892385567239292, 1.7611659577839736, 1.7362430321345, 0.5888954484080826, 0.6060248859065808, 0.601641048821657, 0.6039953317005975, 0.6069990719254527, 0.6054566107289054, 0.6000985876251098, 0.591736824296459, 0.6486455242322278, 0.6484019777275097, 0.6677233337684697, 0.6769781009477531, 0.6766533722747957, 0.6902107943707635, 0.6491326172416637, 0.641257946922449, 0.6671550585907944, 0.6477525203815951, 0.6510809892794076, 0.6493761637463816, 0.6514057179523649, 0.6350069199680207, 0.6418262221001242, 0.6440993228108254, 0.621693044376771, 0.6339515517809095, 0.6508374427746896, 0.658387384420947, 0.6391472105482264, 0.6385789353705511, 0.6385789353705511, 0.6267263388076093, 0.6263204279664126, 0.6264827923028913, 0.6264827923028913, 0.626401610134652, 0.626401610134652, 0.626401610134652, 0.626401610134652, 0.6264827923028913, 0.6260768814616946, 0.6262392457981734, 0.6272946139852845, 0.626888703144088, 1.3324429273120844, 1.3356902140416573, 1.3395057759489057, 1.3440519773703081, 1.3530631980448735, 1.3651593411125333, 1.382694689452228, 1.4002300377919228, 1.4357878274807483, 1.4883126903315933, 1.3917059101267935, 1.3530631980448735, 1.3530631980448735, 1.3452697098938982, 1.2177325235899144, 1.3530631980448735, 1.41143317700895, 1.446341509351861, 1.4763789116004122, 1.502600751941715, 1.5285790457783, 1.6236433647865525, 1.6236433647865525, 1.57322923830993, 3.1754405106813, 3.2077510136405527, 3.312232464164567, 3.117719989063138, 3.2295078347286923, 3.1994704324801413, 3.1854259173747375, 3.2629548880432955, 3.2182235133434256, 3.3240038785592696, 3.3288748086536293, 3.4126548062766155, 3.507962671789586, 3.5293947642047687, 3.6850209807195595, 3.812476984855304, 3.9711069415949503, 4.0824888764193075, 4.333260594110591, 4.629737872520615, 4.55675510327346, 4.78853019359674, 4.8669521681159305, 4.758168062675232, 4.792751666345185, 5.276110296042142, 5.362488123048786, 5.518276703900056, 5.818001269039653, 5.910143029991291, 5.763446851982826, 6.0444995184273775, 6.101164671858428, 6.134286996500074, 6.118618838029884, 6.0888249822860505, 6.02249915083452, 6.049857541531173, 5.811263149075789, 5.280331768790587, 5.882784639294637, 5.901131809316725, 5.914364502739735, 5.870120221049302, 0.2118854591046451, 0.22349450916286895, 0.6155231995905821, 0.6482396133910311, 0.7618946489260898, 0.20474142829958428, 0.40599202336487744, 0.41589624789007545, 0.46744692472204846, 0.4799489786309049, 0.5346657600242117, 0.4869306450994871, 0.9119192958323672, 0.9438238879504229, 1.1052140384102063, 1.2691208360854087, 1.1682114009639246, 1.1741376992453953, 1.3658899806266873, 1.4879067794903966, 1.5924694121826508, 1.6616366195225578, 1.7804061316566941, 1.8878101402373246, 0.6368741098375252, 0.6522987218029975, 0.6716200778439575, 0.6592803882715796, 0.692565077249704, 0.863534723561728, 0.8406413521182375, 0.7284475956114868, 0.9122440245053245, 1.0234635949932034, 0.966636077225674, 0.9457722599881668, 1.0844314033409383, 0.9496690040636546, 0.7148901735155191, 0.7838950165189476, 0.9359492176312082, 0.9856327045936767, 0.9516173761013984, 1.0051164249711153, 1.0614568497292087, 0.9383035005101487, 1.1881822143507992, 1.2267437442644797, 1.2453344607912857, 1.2131051400002728, 1.133627797293971, 1.2069352952140837, 1.2272308372739158, 1.2201679886370942, 1.2618956231121086, 1.2383527943227035, 1.1824182804058068, 1.2177325235899144, 1.2370538796308743, 1.2547515923070478, 1.179739268853909, 1.1964627955112106, 1.2826782581813765, 1.3111731992333804, 1.1847725632847474, 1.2893351959770014, 1.3815581390968776, 1.2584047898778175, 1.2895787424817193, 1.3463250780810094, 1.2644934524957672, 1.2916082966877025, 1.2683901965712547, 1.2856819984062315, 1.2982652344833274, 1.3235128888057583, 1.251585487745714, 1.1768978929655325, 1.2492312048667735, 1.2673348283841435, 1.2752094987033584, 1.2831653511908125, 1.3533067445495914, 1.3076823659990895, 1.291445932351224, 1.2972910484644555, 1.2985899631562847, 1.302649071568251, 1.3030549824094477, 1.302892618072969, 0.9030704394942806, 0.8461617395585118, 0.857364878775539, 0.8467300147361871, 0.8042717407470188, 0.6602545742904515, 0.654165911672502, 0.8878081918652869, 0.9717505538247517, 1.0757449113393305, 1.0792357445736214, 0.9992713088578837, 1.015913653346946, 1.1645582033931547, 1.024843691853272, 1.0360468310702993, 0.9073730944109649, 0.9714258251517943, 0.9034763503354771, 0.81498778695461, 0.8997419705964681, 0.9992713088578837, 0.959167317747656, 0.9378164075007127, 0.834796236005006, 0.8895941995665522, 0.7584038156917987, 0.8344715073320487, 1.0496854353345062, 1.0519585360452073, 1.0422166758564881, 0.7795923616022632, 0.9749166583860854, 0.9504808257460479, 0.8445380961937253, 0.8332537748084587, 0.649700892419339, 0.7933121480347095, 0.8537928633730086, 0.9168714080949663, 0.7050671311585605, 0.7390012774825994, 0.7791864507610665, 0.6061872502430594, 0.6987349220358928, 0.7039305808032098, 0.7011703870830727, 0.7769945322186047, 0.6773028296207104, 0.6630147680105887, 0.6754356397512058, 0.6796571124996509, 0.6853398642764038, 0.5827256036218937, 0.6981666468582176, 0.620231765348463, 0.42003653847028116, 0.6820113953785913, 0.7230895725076911, 0.7994819928208985, 0.7619758310943291, 0.8895941995665522, 0.8222941820961496, 0.8030540082234289, 0.9910719098657117, 0.8459993752220332, 1.0880034187434688, 1.0776932833770743, 1.0992877401287353, 1.1277826811807394, 1.0729847176191933, 1.0828077599761519, 1.0529327220640794, 1.0628369465892773, 0.962901697486665, 1.0951474495485296, 1.133627797293971, 1.1148747164306863, 1.1627721956918895, 1.083376035153827, 1.069169155711945, 1.2973722306326947, 1.3474616284363599, 1.4728880783661211, 1.294206126071361, 1.4353819166395518, 1.371004457225765, 1.3828570537887068, 1.1650452964025908, 1.1707280481793436, 1.1669936684403346, 1.1657759359167448, 1.168617311805121, 1.180957001377499, 1.1669124862720952, 1.1437755683238868, 1.1645582033931547, 1.1629345600283683, 1.1777908968161652, 1.169266769151036, 1.1824994625740461, 1.19654397767945, 1.2189502561135044, 1.2316958565270788, 1.240950623706362, 1.2476887436702262, 1.2118874074766828, 1.1812817300504563, 1.1543292501949995, 1.1305428749008766, 1.1408530102672712, 1.1546539788679568, 1.1810381835457382, 1.1843666524435508, 1.1878574856778419, 1.1919165940898082, 1.1963816133429712, 1.2113191322990076, 1.2039315549892287, 1.2073412060552804, 1.2053928340175366, 1.210669674953093, 1.214160508187384, 1.2193561669547008, 1.2290980271434202, 1.189156400369671, 1.2048245588398614, 0.9619275114677931, 0.9957804756235926, 0.9357868532947295, 0.9663113485527167, 1.0117733627667402, 0.540186147464486, 0.5898696344269545, 0.539455507950332, 0.5019493462237627, 0.6155231995905821, 0.6378482958563971, 0.5694929101988833, 0.506495547645165, 0.5177798690304316, 0.5676257203293787, 0.6455606018391333, 0.5579650423088988, 0.5485479107931368, 0.7188680997592461, 1.009987355065475, 1.1164171776272336, 1.2420871740617128, 1.436437284826663, 1.234537232415455, 1.242655449239388, 1.3106861062239445, 1.1704033195063863, 1.1622039205142143, 1.1323288826021418, 1.138336363051852, 1.109354328990412, 0.9013656139612546, 0.9312406518733272, 0.8418590846418275, 0.8905683855854241, 0.8157996086370033, 0.8664572816183438, 0.8938968544832365, 1.0458698734272578, 1.2454156429595251, 1.3481110857822745, 1.0927931666695891, 1.209939035438939, 1.3205903307491424, 1.3407235084724958, 1.2485005653526196, 1.190861225902697, 1.0826453956396733, 1.0729847176191933, 1.0311759009759396, 1.0444085943989498, 1.0604826637103368, 1.0977452789321882, 1.1339525259669283, 1.2849513588920776, 1.258973065055493, 1.1442626613333229, 1.0413236720058554, 1.0080389830277312, 1.2677407392253401, 1.5933624160332833, 1.3605319575228916, 1.0903577016224093, 1.3263542646941346, 1.5312580573301977, 1.2733423088338538, 1.4171159287857031, 1.398768758763615, 1.2680654678982974, 1.3901634489302463, 1.3485169966234711, 1.1862338423130552, 1.5572363511667826, 1.4232045914036526, 1.1386610917248092, 1.1342772546398856, 1.2907964750053094, 1.1590378159528805, 1.1162548132907548, 1.3310628304520158, 1.2082342099059131, 1.2940437617348823, 1.465906411897539, 1.3363396713875721, 1.33658321789229, 1.4394410250515182, 1.150757234792469, 1.0089319868783637, 1.3882962590607417, 1.2295851201528563, 1.4164664714397883, 1.3488417252964284, 1.3213209702632964, 0.5056025437945324, 0.3045143130657179, 0.4773511492472464, 0.7383518201366848, 0.5707918248907126, 0.5196470588999361] A [1.6978625072212594, 0.8251842751842752, 0.6142075143070416, 0.9390795926065636, 1.3940293584034307, 1.7095815795989107, 0.6507760532150776, 0.8311117413499717, 0.9131566108310294, 1.19278431372549, 1.0993535216059884, 0.8832812267381271, 1.1737897508608466, 1.0994175537256476, 1.0796261239943208, 1.2124547429398986, 1.849355074227306, 0.9795256782119093, 1.3542297417631344, 0.9043924250394529, 1.7248529866914268, 1.5899207820664083, 1.2798964624676445, 1.3690171720862259, 0.9090410958904109, 1.5979455327281413, 1.0261876562705619, 1.4741334262323638, 1.4070226196738558, 1.2844263486985603, 0.9930916920868473, 1.1222304675076857, 1.9823378724551706, 1.7935681878836403, 1.809282700421941, 1.123934818358499, 1.6074634521672224, 1.2217889637244477, 1.545845406112721, 1.5862108004075626, 1.303098744240672, 1.5847345550727376, 1.2331496973406788, 1.0186742057882598, 0.9730676328502416, 0.9397808021197158, 0.9569138276553106, 0.8136560858643526, 0.8303648068669528, 0.9407982530632052, 0.7403633685244292, 0.8279756733275413, 0.5664701301590833, 0.6780423570723143, 0.559885813578255, 0.5774702037677816, 0.6342931937172774, 0.6539433062026382, 0.5803075332730326, 0.4897485493230174, 0.6320676504725584, 0.4982761205216609, 0.5447761194029851, 0.5195015374656093, 0.9255154067834183, 0.8812694185530404, 0.5530334296326868, 0.640343347639485, 0.8940102427076375, 1.7195892575039495, 1.4881951731374607, 2.1107099879663056, 2.236481614996395, 1.7582554517133957, 1.4021556886227544, 1.579451019894233, 2.410820895522388, 1.647788203753351, 1.4530510585305105, 0.6827438370846731, 0.8184382161114049, 0.9303021664766249, 0.9210186976144423, 1.1149893692416726, 1.1315339938503588, 0.8801020408163265, 1.2604859928803591, 1.8065893837705918, 0.9754623695293902, 1.1345172484180446, 1.3658391261171798, 1.3374267901001322, 1.34052583862194, 1.362542309138774, 1.3056277056277057, 1.2915650052283025, 1.2947198275862069, 1.2446716340322301, 1.2273324572930355, 1.1685858222382153, 0.8723280500218118, 1.1288317558977257, 1.157581496009739, 1.1540160088324594, 0.9616710875331564, 1.1262987451086224, 1.1842141767235401, 1.180426005847139, 1.3438360354848577, 1.4063125481139338, 1.5322553758959827, 1.1974721561756976, 1.269572329983641, 1.2073666586940923, 1.5404771755619915, 1.2650053911585, 1.3618367107883331, 1.2997995046585682, 1.217049852037332, 1.0347054959492008, 1.2496736292428199, 1.1982443306510606, 1.0459272894615739, 1.1149960380348654, 0.9074306177260519, 1.18193010701771, 1.0934113194563908, 1.0893748298702477, 1.4194332741045543, 1.4529679399005395, 1.218159658744668, 1.273591487877202, 1.3107080163838503, 2.792991755005889, 0.7199582027168234, 2.504530627038782, 1.3529411764705883, 1.4621504887474426, 1.309876932786368, 1.0271406727828747, 0.4273255813953488, 3.7531645569620253, 0.6142857142857143, 0.30353189246178175, 0.9626269956458636, 0.21808972503617946, 0.4497982967279247, 0.308613184079602, 0.4594073717176584, 1.39784145887607, 2.183673469387755, 1.119308600337268, 1.385174418604651, 1.002778742618965, 1.598567659253675, 1.484406104844061, 2.2835077229696066, 1.6287153652392947, 1.4509701101206083, 0.6568157614483493, 0.7930079612322603, 0.575894538606403, 1.454354669464848, 1.2390024246622793, 2.863947182268333, 1.2293249888243183, 0.9581060440759328, 1.1531085678935973, 2.3100831225153597, 1.4657478719092014, 1.5888258402444349, 2.431000654022237, 0.8158369408369408, 1.0265585686329326, 0.17915363082496294, 1.1069090909090908, 1.08812729498164, 0.9650554509311572, 0.92284929190446, 0.9343540669856459, 1.1490632077414042, 1.0080742834073475, 1.1691355295158081, 0.921061041866411, 1.1973168214654282, 1.102981029810298, 1.2971878515185602, 1.2322202948829142, 1.179340357306459, 1.2083162638263008, 1.27002329331661, 1.4006808169803764, 1.213502269288956, 1.2611034004163775, 1.159972418548526, 1.0533520533520533, 1.5202913631633714, 1.5335210276262536, 1.8182171295397165, 1.3356501101881675, 1.2007618510158014, 1.3102215868477485, 1.097865045971638, 1.2230017884165636, 1.3566651805617476, 1.4311896034655116, 1.10403832991102, 1.0816982214572577, 1.06419568468276, 1.193933240258916, 1.3244037128422042, 1.3135842880523732, 1.3287437899219305, 1.1697412823397075, 1.097601051593822, 1.2371362048894063, 1.02820830750155, 1.2761217778720695, 1.160794941282746, 1.3030721802912724, 1.173853307968717, 1.0140510587769642, 1.2070465419747716, 0.9812481096884766, 1.1231191541276941, 1.1321992211518754, 1.1131053845382533, 1.180262025437506, 1.0313144571535722, 0.9736336171381489, 0.9727199063653552, 1.049375487900078, 0.9855855855855856, 1.1393198397205384, 1.0432696659726624, 1.098388848660391, 1.106386811998919, 0.967671366067088, 1.2445454545454546, 1.2679830747531735, 1.2670307293594965, 1.2938441510135763, 1.273400365630713, 1.1489764631616917, 1.1556616052060737, 1.142233209723939, 1.1673994463442436, 1.0683245415724691, 0.9565376186997808, 0.914423433444568, 0.9376141427423479, 0.9406353313209717, 0.9246285263082334, 1.0423828584883448, 1.0093100082588784, 0.9212899502200419, 0.9061933324034035, 1.0038404263657026, 0.9896143566246659, 0.9914848755169897, 0.9068952084144917, 0.8455837408312958, 0.9146805372253707, 1.0028612303290414, 0.4666153846153846, 0.2949230769230769, 0.36615384615384616, 0.48353846153846153, 0.9149680872966852, 0.9328817733990148, 0.767028022090407, 0.7575, 0.9796571428571429, 1.725, 1.4131223211341906, 2.2952529994783517, 1.6205882352941177, 1.5704740693604835, 0.9021152115211521, 1.1755775577557757, 1.6192, 1.4458445844584458, 0.6388240783947737, 0.6297760210803689, 0.5900606626224918, 0.6865909090909091, 0.9310344827586207, 0.5070907617504052, 0.6557745073584434, 0.5936739659367397, 0.24193017127799737, 0.29362938368956215, 4.243973703433163, 2.375373580394501, 2.519968366943456, 0.4915590863952334, 1.0559732664995822, 1.5389532560926888, 0.9250665652339293, 0.4435687263556116, 0.7494894486044928, 2.380918727915194, 0.19862306368330465, 0.27679919476597886, 0.2789894868978503, 1.3212121212121213, 0.48074894514767935, 0.5238836967808931, 0.9315893043176293, 1.0827722772277228, 0.9939386123291205, 1.1278935925310143, 1.376008326827999, 1.0546894031668697, 1.1351315789473684, 1.0893112875927613, 0.9750775101359409, 1.0819979969954931, 1.0748011928429424, 1.085466959278225, 1.1621902166861295, 1.220773330309559, 1.0912443267776097, 1.595449820995496, 1.4234380433522662, 1.3934504601410302, 1.3677387795034854, 1.4292491033206063, 1.3634682080924856, 1.2043131528698192, 1.0710059171597632, 0.8998699609882965, 1.015596568754874, 0.9625769091567137, 1.1401465798045602, 1.2072218886696973, 1.6927753934191703, 1.4275884400550474, 1.448957096828896, 1.5693900391717963, 1.688627123944543, 1.6135425268373245, 1.424451838580326, 1.278613325312077, 1.1972716234554674, 1.1925399644760213, 0.9100464821466301, 0.9313864474057273, 1.002340686991632, 0.9458013906222142, 1.034631767393049, 1.0753582395087, 1.6205417607223476, 1.2871140691440193, 1.2553778268063982, 0.7596735187424426, 0.6751455327765122, 0.6521200099181751, 0.7172271791352093, 0.7314496314496315, 0.6347504149112728, 0.7313307130825379, 0.7934252681431955, 0.8033611031312841, 0.9879906268306972, 1.5362116991643453, 2.0185567010309278, 2.2610266159695818, 1.8342583732057416, 1.8681558616056433, 2.16934835076428, 1.5506717850287908, 1.5686446629213484, 1.798855712497765, 1.4675363178179661, 1.1353451625437119, 0.8074101587891169, 0.7423694610274951, 0.9890442404006677, 0.8861817854895262, 0.9335249397366957, 1.229607624706028, 1.1557918298824845, 1.041944140741477, 1.0197979797979797, 1.161989504905316, 1.1767682576193215, 1.1322913127194474, 1.0034813798923936, 0.9605356599371005, 0.9419008839010825, 1.4570163076303604, 1.4681901810787257, 1.5991605456453306, 1.567056259904913, 1.4524838012958963, 1.0721040189125295, 0.8869693071126631, 0.6758502504613763, 1.016052880075543, 1.0589096231247712, 0.7754668286017914, 0.7824461722488039, 0.770058269834155, 0.6760621476694624, 0.7745500921071277, 1.1370294534572374, 1.4003315375051804, 1.0193095377413692, 0.780359355638166, 1.0784381478921907, 1.1433046202036021, 1.0592394420026754, 1.4249126891734576, 1.1169412243237746, 1.0230515916575191, 0.856905388557972, 1.0800532701982835, 0.9586681974741676, 1.481738785232235, 0.9380006408202499, 1.0296232876712328, 1.0499729388417824, 0.9458061002178649, 1.2063000728332118, 0.9451001430615165, 1.02602651147753, 0.9191670091793397, 1.5778443113772456, 0.8991292699263228, 1.3009393680614858, 1.2464659903542326, 1.3733676975945017, 1.1687302044342067, 0.9030943396226415, 1.4433301797540208, 0.9957460217425556, 1.3884334476076912, 1.0408602150537634, 1.274880810488677, 0.8440151651922729, 0.900965319985699, 0.9953861769862508, 0.9719545767377838, 1.0987304776691935, 1.0647832888561701, 1.0966069316837987, 0.977114009255364, 0.9646501457725948, 0.9640123906705539, 1.131336898395722, 1.0497023809523809, 1.0429220357838138, 0.9824747742963357, 0.9050706566916044, 0.9660357791029297, 0.9585234342596433, 0.99655558425902, 1.2101435587457499, 1.2318306398261034, 1.2310455662696163, 1.2211511199319536, 1.1474666666666666, 1.164954954954955, 1.1893828067597354, 1.1587940597602433, 1.1190826482042406, 1.1054177827702412, 1.0148286896121126, 1.0094368574497468, 1.005605897711565, 1.0123055491061064, 1.0147958788442173, 1.0141520377387674, 1.0125096525096524, 0.9871072338454412, 1.033717423246462, 1.006487923082936, 1.0173036991463509, 1.0354184084517748, 1.0348224513172968, 1.008868501529052, 1.0561068702290077, 1.0443800518529816, 1.3333333333333333, 1.257275411218895, 0.7411366859386931, 0.7572378297233541, 0.6347510780086241, 0.6482, 0.603813365111376, 0.33951624870158775, 0.7241705165896682, 0.9510631117111036, 1.0269844603107938, 0.7231354131723136, 1.098322048866647, 0.9599263664684226, 1.1854253512428594, 1.318420240666461, 1.1694246242599058, 1.7524038461538463, 1.2105263157894737, 0.7384669978708304, 0.6494223494530211, 0.9884026596567187, 0.8663897078531225, 0.9653341200767075, 0.958061995311279, 0.8732740463374679, 0.968843307802155, 0.9425115351041277, 0.8820217990178465, 1.2075925036040365, 1.220088161209068, 1.2077596996245306, 1.2080433101314771, 1.1871943765281174, 1.0475802066340403, 1.0422082272544553, 1.0238509982580732, 1.0206403810531888, 1.0746876697447039, 1.0407215811115533, 1.0250322580645161, 1.049222797927461, 1.0642765685019207, 1.060368129746428, 1.6025312399871836, 1.048491790759832, 1.1179678148689984, 1.1422118983094594, 1.0445576066199873, 0.8912810752943975, 0.8479019012262347, 0.8581681476418318, 0.9023784167554135, 1.1192372676804248, 0.8975307407777666, 1.151978635591163, 0.9334998810373543, 1.0968252174895492, 1.2025594149908592, 1.1363939677031896, 1.1245853787979303, 1.1749634939599096, 1.1574875426173616, 1.1995902523183093, 1.1726442414791713, 1.1072708113804004, 1.5743596278832674, 1.1650185414091472, 1.1570892875240701, 1.6310041103934234, 1.5180509674374705, 1.6931420178510903, 1.8643933386201428, 1.512179775280899, 1.6336436170212767, 1.6662542824514655, 1.4788732394366197, 1.6391688770999115, 1.6083909958833318, 1.3161722350230414, 1.2197093339550789, 1.1577472307486987, 1.0399829859634198, 1.3499970278784996, 1.0955288098145242, 1.1884173853446798, 1.1874110563765736, 1.1536220939640758, 1.037471395881007, 1.065211187214612, 1.0709537572254335, 1.0749891634156914, 1.1574303405572754, 1.0729391454466983, 1.0563018005144327, 1.0258972554539056, 1.1240855374226224, 1.0984733874295145, 1.1338571822442791, 1.1170797052913597, 1.1246795304277426, 1.142741935483871, 1.0694128661227764, 1.0591311343523733, 1.1117424242424243, 1.0946631911098916, 1.0037546933667083, 1.0015024414673845, 0.9755623100303952, 0.9380021585321981, 0.9485302939412118, 0.933192190072924, 0.957728864432216, 0.9886061526775541, 0.9755414942808469, 1.0164180975059531, 0.9816708229426434, 0.9833729216152018, 0.980309072781655, 0.986959856814114, 0.9758411333164685, 0.9726493571968742, 1.0077043614520762, 0.9880906646177487, 0.9624547835848821, 0.9514180024660912, 0.9800584275371523, 0.9810577167556572, 0.9804220696669209, 0.9992227979274612, 1.0015554115359688, 1.0006479201762344, 0.65652, 0.65812, 0.66, 0.7095529936141944, 0.9524, 1.0089398212035758, 1.1354666666666666, 1.1210919727006825, 1.326483162079052, 1.0999580008399832, 1.044476939011759, 1.0130067464900019, 1.0101212121212122, 1.0009060159458807, 0.8999820003599928, 0.991139391056137, 1.0207844058243307, 1.032931354359926, 1.0282709487730408, 1.009600174548628, 1.0983491804234964, 1.1999760004799904, 1.1999760004799904, 1.1694526582584033, 1.346147227862477, 1.2122039514050804, 1.2315865732914755, 1.1532386414822378, 1.053633859519017, 1.1053428691628102, 1.1030275770949878, 1.1081304623528438, 1.0666200290588173, 1.0467048417608262, 1.048319059184456, 1.0638777111330449, 1.059093137254902, 1.1320435371315487, 1.1410472336039819, 1.1915962548527061, 1.246648656914216, 1.2511631378598262, 1.3464759598405731, 1.3928196360972036, 1.3688574564077551, 1.4031686371529843, 1.387401356136169, 1.3481541115583668, 1.3006036305956996, 1.3839061368766237, 1.3503761550412954, 1.351694241170856, 1.3426382149615004, 1.2765610478879166, 1.2648138250489933, 1.2622870221242688, 1.2535904321862854, 1.2892119226766308, 1.2766400731744498, 1.154036712775615, 1.1230792521383695, 1.096330950069144, 0.998841849691625, 0.8934355297317345, 1.0207059751528298, 0.9762812936499409, 0.9693828671793916, 0.9569360260448374, 4.81549815498155, 0.3589308996088657, 1.822596153846154, 3.01434503586259, 3.534839924670433, 0.2627904553506304, 0.6698365925529065, 1.3581654294803818, 0.9037827656568828, 3.737041719342604, 2.5233716475095784, 2.1787141300399564, 0.8555217060167555, 1.1930220625962031, 1.3172714078374457, 1.2881509558338826, 1.138539441411504, 1.267683407835919, 1.4960874977769874, 1.641852548598047, 1.6373956594323873, 1.5120041368102237, 1.9523724739606516, 2.000172028212627, 1.0978169605373636, 1.1440979638331197, 1.1751420454545454, 1.1224602626123013, 1.164959715963403, 1.4354925775978407, 1.3910531972058033, 1.2388513047079939, 1.4754464285714286, 1.636210253082414, 1.5177820267686424, 1.449906658369633, 1.6146500664813248, 1.4404629971678364, 1.0322353768608603, 0.9077747485193194, 1.1133751810719459, 1.3530591775325977, 1.0431609860282993, 0.982073451257238, 1.0980935584110187, 0.9921030042918455, 0.9221270161290323, 0.9981504722901117, 0.9598898692196984, 0.9372765477011855, 0.8713340821165606, 0.9827472236911687, 0.997887649349792, 0.9996009577015164, 1.0346801570924582, 1.0408734220402593, 0.9951489477999453, 0.9926543577526306, 0.9933507170795306, 1.0343304557317807, 1.0406760240618733, 0.9913230645052802, 1.0451809221406363, 1.0745841650033268, 0.9388831703551209, 1.0411695293037893, 1.1684174390662547, 1.0334, 1.0424596403727524, 1.0729813664596273, 1.0718414533443434, 1.0795223232460307, 0.9888607594936709, 0.9805584793511237, 1.0957927915581747, 1.0265079964739958, 0.9059231402044894, 0.9352299851622476, 0.9687126219704123, 0.9413289917993246, 1.0084745762711864, 0.993463230672533, 1.06694828469022, 1.0171118267348613, 0.9947473736868434, 0.9801876955161627, 1.03755594473633, 1.1068496930399394, 1.0430855211853391, 1.0280571391967204, 1.0345019994420162, 0.9646459972235076, 1.0164581328200193, 0.958904109589041, 0.9311090225563909, 0.804371476609633, 0.8391127772571072, 1.051134179161861, 1.177685950413223, 1.194752502028672, 1.1950737144911903, 1.1809459848412165, 1.1849256699176214, 1.3753595397890699, 1.2742505299283335, 1.5691626706012543, 1.3870687515512534, 1.0941843452816387, 0.9297410192147034, 0.7576031997585088, 0.8336843688882203, 1.0, 0.9441425603324277, 0.8052980132450331, 0.8145595690747782, 0.8586428459489108, 0.835823566252125, 0.8590172154437573, 1.1618294545781291, 1.290766012551051, 1.158350627086529, 0.7801608579088471, 1.0164198053322049, 1.0135041551246537, 1.0116697461830204, 0.9366672750501917, 0.8566688075358596, 0.9506761358108765, 0.8133797370456303, 0.871585121160673, 0.6765072441190216, 0.9479329376236593, 0.7992339079024066, 0.637769046805603, 1.0542626163645272, 1.0710227272727273, 1.095787871098706, 1.230838477366255, 1.0310182896688087, 0.7501607421695601, 0.9081977950005458, 0.9742813918305597, 1.0235208535402522, 0.8364017711489163, 0.999186708493087, 0.8810979125821704, 0.5990505962718536, 0.8777557204053913, 1.067601582164689, 1.2058283335374067, 1.128125, 1.3088867654085046, 1.1998341625207296, 1.3780997492337699, 1.4195348837209303, 1.3640052356020942, 2.590258987243912, 1.5801690274967266, 1.5202649601437073, 1.4106417546709993, 1.4081610909865758, 1.2171929184157693, 1.2804817849738375, 1.3234937323089364, 0.9715760157273918, 1.2945014873812495, 1.0419340396955679, 1.0345009416195856, 1.057750535410974, 0.960624820040311, 0.9964439736702732, 1.1981556455240665, 1.279722436391673, 1.3858081271005194, 1.3440687968973948, 1.3106745737583394, 1.2093955886565455, 1.2403699118910654, 1.0709701492537314, 1.053627529772777, 1.1438688628948834, 1.0529403138290072, 1.1041650686507631, 1.0445928479103834, 1.0196495708306732, 0.9981579879560751, 1.0156471254602095, 1.0174728318772641, 1.0109400041808934, 0.9987518202621177, 1.013286956521739, 1.026392757660167, 1.0430705105939562, 1.042964185055338, 1.0634478920272714, 1.0908510185250906, 1.0406413384454514, 1.0157766143106457, 0.9800799558864075, 0.9668818996042491, 0.9647809968419607, 0.9649908406269082, 0.9688977688977689, 0.9615739520168732, 0.9572157529765799, 0.9552996291235604, 0.9872052518756699, 1.0254278056490962, 1.0429706730431114, 1.0679304897314377, 1.0565715505585995, 1.0485129719468467, 1.0280450921088808, 1.0295428062238674, 1.0347184253690542, 0.997684239204468, 1.007057067245708, 0.7941156758930367, 0.8271072151045179, 0.775080688542227, 0.8016567887931034, 0.8357138067457922, 0.8170432220039293, 0.9231355609198323, 0.8389092286327484, 0.9959729381443299, 0.9976315789473684, 1.0087302606239568, 0.8308658059931304, 0.6922223455009431, 0.9264962231260895, 0.8771797766905031, 1.1950706342049895, 0.9459124690338563, 1.016854778028593, 1.4321526766941615, 1.6408599314165129, 1.7502863688430699, 2.18104062722737, 2.836031415290912, 2.3842897460018815, 2.189216247139588, 2.0303068410462775, 2.097628400989379, 2.1186917270978243, 1.5751552795031056, 1.1270798167349891, 0.993673647469459, 0.7256862745098039, 0.648298858370069, 0.6819227987111198, 0.7166655778402038, 0.6224218024156085, 0.7403065825067628, 0.7691394244202291, 0.9236449670203614, 1.0940664669804592, 1.2152213684595683, 1.2123750337746555, 1.2992764362304943, 1.568659594985535, 1.5054694621695532, 1.5304010349288486, 1.3744026983978264, 1.2111524838797567, 1.025925638438252, 1.0204868643046516, 1.0661307698682356, 1.0452908698087542, 1.1272090696898966, 1.2220472440944883, 1.3885428546363716, 1.2554035456974015, 1.0954379420222273, 0.9844205679201842, 0.9714442184321702, 1.2294126909148166, 1.525612125923047, 1.282936538314323, 0.993270226297885, 1.1696735395189004, 1.1916856204195097, 1.011413464018571, 1.2384533522525718, 1.3432603102829968, 1.2579528066360635, 1.0965676229508197, 0.8463341315534723, 0.8718897308908646, 1.42818851909761, 1.0730199534826783, 0.7436114940091189, 0.8907873764743386, 0.9108615948670944, 0.8286128845037725, 0.8802816901408451, 0.9574865685587479, 0.89597254831136, 1.0908842047632084, 0.9413512668126368, 0.9389652615367065, 1.1738200484813917, 1.2690380761523046, 0.8915094339622641, 0.8704909995097009, 1.2437090909090909, 0.9237618931446694, 1.1723442854263253, 1.0423462986198244, 0.9013678905687545, 0.7447978952403731, 0.4604148766417086, 0.7229804500184434, 1.1233942687747036, 0.9297804813541392, 0.8008257225071937] hay\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#variance_depth=\n",
    "r_asumb=1\n",
    "q=1\n",
    "lrn_r_depth=8\n",
    "#nmbr_pnts=\n",
    "#rndmnss=\n",
    "nmbr_data_points=50\n",
    "lrning_depth=5\n",
    "h_lim=0.01\n",
    "intial_est=2\n",
    "data_randomness=5\n",
    "min_step=-2\n",
    "lrn_r_depth=0\n",
    "predicted_randomness=0.4079937249825579\n",
    "euler_est=2\n",
    "r_asum=0\n",
    "qsum=1\n",
    "\n",
    "\n",
    "\n",
    "car=pd.read_excel(\"C:/Users/dylan/Downloads/censusdata.xls\", index_col=0)\n",
    "car=car.drop(['Percent home ownership'], axis=1)\n",
    "car=car.drop(['Percent under 18 years old'], axis=1)\n",
    "car=car.drop(['Percent over 25 with bachelors degree'], axis=1)\n",
    "car=car.dropna()\n",
    "inputs=car[\"Percent over 65 years old\"].values\n",
    "y=car[\"Auto Fatalities per million miles \"].values\n",
    "print(y)\n",
    "dfx_crp=pd.DataFrame()\n",
    "dfy_crp=pd.DataFrame()\n",
    "X=[]\n",
    "y=[]\n",
    "for i in range(0,56):\n",
    "    df_crp[i*2]=df_crp[i*2].shift(periods=1)\n",
    "for i in range(0,56):\n",
    "    dfx_crp[i]=df_crp[i*2]\n",
    "    dfy_crp[i]=df_crp[i*2+1]\n",
    "for k in range(0,56):\n",
    "    X.append(dfx_crp[k])\n",
    "combinedx = pd.concat(X, ignore_index=True)\n",
    "\n",
    "for k in range(0,56):\n",
    "    y.append(dfy_crp[k])\n",
    "combinedy = pd.concat(y, ignore_index=True)\n",
    "#dfx_crp=dfx_crp.stack(dropna=False)\n",
    "#dfy_crp=dfy_crp.stack(dropna=False)\n",
    "df_concat = pd.concat([combinedy, combinedx], axis=1)\n",
    "df_concat=df_concat.dropna(inplace=False)\n",
    "inputs=df_concat[df_concat.columns[0]].head(1000)\n",
    "y=df_concat[df_concat.columns[1]].head(1000)\n",
    "y=y.values\n",
    "inputs=inputs.values\n",
    "yd=[]\n",
    "for i in range(0,len(y)):\n",
    "    yd.append(inputs[i]/y[i])\n",
    "xd=[]\n",
    "x_AV=(sum(inputs)/len(inputs))\n",
    "for i in range(0,len(y)):\n",
    "    xd.append(inputs[i]/x_AV)\n",
    "inputs=xd\n",
    "y=yd\n",
    "\n",
    "print(inputs)\n",
    "#inputs=np.random.randint(-1000, 1000, size=nmbr_data_points)/100\n",
    "#y=inputs**2\n",
    "b1=y\n",
    "#plt.scatter(inputs, y)\n",
    "#a=data_randomness\n",
    "#r1=(1+a)*1000\n",
    "#r2=(1-a)*1000\n",
    "#r=np.random.randint(r2, r1, size=nmbr_data_points)/1000\n",
    "#y=y*r\n",
    "#b1=y\n",
    "x_values=np.random.randint(0.03, 8, size=nmbr_data_points)/10\n",
    "t_min=[]\n",
    "e=10\n",
    "t=[]\n",
    "y_avg=sum(y)/len(y)\n",
    "y_var=(sum(abs(y_avg-y)))/len(y)\n",
    "yovar=y_var\n",
    "for q in range(0,lrn_r_depth): \n",
    "    \n",
    "\n",
    "    y_var=(yovar/(1+(predicted_randomness)))\n",
    "    print(\"y_var:\",y_var)\n",
    "\n",
    "    def rosen(x):\n",
    "        if x<1:\n",
    "            x=1\n",
    "\n",
    "        t=[]\n",
    "        t_min=[]\n",
    "        for i in inputs:\n",
    "            y_xa=sum(y*(1/(x**(abs(i-inputs))))/sum(1/(x**(abs(i-inputs)))))\n",
    "            t.append(y_xa)\n",
    "        for i in x_values:\n",
    "            y_xb=sum(y*(1/(x**(abs(i-inputs))))/sum(1/(x**(abs(i-inputs)))))\n",
    "            t_min.append(y_xb)\n",
    "\n",
    "        t_avg=sum(t)/len(t)\n",
    "        t_var=(sum(abs(t_avg-t)))/len(t)\n",
    "        return(abs(t_var-y_var))\n",
    "        print(t_var-y_var)\n",
    "#        return(abs(t_var-y_var),((((sum(abs(t_min-y)))/len(t_min))),t_var,t_var-y_var))\n",
    "    #\n",
    "\n",
    "    \n",
    "    ####minimizer\n",
    "    x0 = np.array([2.0])\n",
    "    #    \n",
    "    #res = minimize(rosen, x0, method='nelder-mead',\n",
    "    #            options={\"maxiter\":500, \"maxfev\":500})\n",
    "    res = minimize(rosen, x0, method='nelder-mead',\n",
    "            options={\"maxiter\":500, \"maxfev\":500})\n",
    "    intial_est=res.x\n",
    "#    for i in range(0,lrning_depth):\n",
    "#        der1=((rosen(intial_est+h_lim)))[0]\n",
    "#        der2=(rosen(intial_est))[0]\n",
    "#        intial_est=intial_est+(((((rosen(intial_est+h_lim)[0]-rosen(intial_est)[0])))/h_lim))*((((min_step))))\n",
    "    if intial_est<-2:\n",
    "        intial_est=(np.random.randint(-200, 200)/10000)\n",
    "        print(\"no\")\n",
    "    print(\"intial_est:\",intial_est)\n",
    "    \n",
    "    #    print(rosen(intial_est),intial_est)\n",
    "\n",
    "    #    if intial_est<1:\n",
    "    #        intial_est=1.1\n",
    "    #        print(\"no\")\n",
    "\n",
    "    #    print(rosen(intial_est),y_var)\n",
    "    #    print(intial_est)\n",
    "    e=abs(intial_est)\n",
    "    r_est=[]\n",
    "    for i in inputs:\n",
    "        y_e=sum(y*(1/(e**(abs(i-inputs))))/sum(1/(e**(abs(i-inputs)))))\n",
    "    y_avgb=(sum(numpy.absolute(y)))/len(y)\n",
    "    r_a=(sum(abs(y/y_e-1)))/len(y)\n",
    "    r_est=(y_avgb/r_a)\n",
    "    \n",
    "\n",
    "    \n",
    "    e=abs(intial_est)\n",
    "\n",
    "\n",
    "    r_asumb=r_asumb+r_a\n",
    "    #rosen1=var_rose\n",
    "    \n",
    "    \n",
    "    r_asum=r_a+r_asum\n",
    "    qsum=1+qsum*0.9\n",
    "    r_a=(r_asum/qsum)*0.8+r_a*0.2\n",
    "    predicted_randomness=r_a+1\n",
    "    print(\"predicted randomnessw:\",r_a)\n",
    "    \n",
    "randomness_estimate=r_asum/qsum\n",
    "tg=[]\n",
    "print(randomness_estimate,\"actual\")\n",
    "x2_values=np.linspace(start = 0.03, stop = 2.5, num =1000)\n",
    "for i in x2_values:\n",
    "    y_x=sum(y*(1/(e**(abs(i-inputs))))/sum(1/(e**(abs(i-inputs)))))\n",
    "    tg.append(y_x)\n",
    "\n",
    "\n",
    "plt.scatter(inputs,y)\n",
    "plt.scatter(x2_values,tg)\n",
    "plt.show()\n",
    "ar=rosen(e)\n",
    "print(ar)\n",
    "#print(r_est)\n",
    "target_column =pd.DataFrame(y)\n",
    "predictors = list(set(list(inputs))-set(target_column))\n",
    "y=numpy.array(y).reshape(-1,1)\n",
    "X =numpy.array(inputs).reshape(-1,1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "\n",
    "# Alpha (regularization strength) of LASSO regression\n",
    "lasso_eps = 0.0001\n",
    "lasso_nalpha=20\n",
    "lasso_iter=5000\n",
    "# Min and max degree of polynomials features to consider\n",
    "degree_min =2\n",
    "degree_max =8\n",
    "for degree in range(degree_min,degree_max+1):\n",
    "    model = make_pipeline(PolynomialFeatures(degree, interaction_only=False), LassoCV(eps=lasso_eps,n_alphas=lasso_nalpha,max_iter=lasso_iter,\n",
    "normalize=True,cv=5))\n",
    "    model.fit(X_train,y_train)\n",
    "    test_pred = np.array(model.predict(X_test))\n",
    "    RMSE=np.sqrt(np.sum(np.square(test_pred-y_test)))\n",
    "    test_score = model.score(X_test,y_test)\n",
    "print(list(model.predict([[i]])))\n",
    "z=[]\n",
    "for i in x2_values:\n",
    "    z.append(list(model.predict([[i]])))\n",
    "print(len(inputs),len(b1),inputs,\"A\",b1,\"hay\")\n",
    "\n",
    "#plt.plot(x2_values,(x2_values)**2)\n",
    "plt.scatter(inputs,b1)\n",
    "plt.scatter(x2_values,z)\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 3, 3\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267.07046106571414\n",
      "1.0937740825213738\n",
      "Lasso Accuracy= [0.1591056]\n",
      "X-axis Accuracy= 0.1534174668477557\n"
     ]
    }
   ],
   "source": [
    "\n",
    "e=abs(intial_est)\n",
    "Xaxis_pred=[]\n",
    "lasso_pred=[]\n",
    "#y_testB=y_testB.tolist()\n",
    "#inputs_test=inputs_test.tolist()\n",
    "for i in range(0,len(y_testB)):\n",
    "    lasso_pred.append(list(model.predict([[inputs_test[i]]])))\n",
    "    a=sum(b1*(1/(e**(abs(inputs_test[i]-input2)))))\n",
    "    b=sum(1/(e**(abs(inputs_test[i]-input2))))\n",
    "    Xaxis_pred.append(a/b)\n",
    "#print((((y*(1/(e**(abs(inputs_test[1]))))))))\n",
    "\n",
    "#print(b)\n",
    "print(a)\n",
    "\n",
    "resultX=0\n",
    "resultL=0\n",
    "print(Xaxis_pred[1])\n",
    "for i in range(0,len(y_testB)):\n",
    "    resultX=resultX+abs(Xaxis_pred[i]/y_testB[i]-1)\n",
    "    resultL=resultL+abs(lasso_pred[i]/y_testB[i]-1)\n",
    "\n",
    "L_accuracy=resultL/len(y_testB)\n",
    "X_accuracy=resultX/len(y_testB)\n",
    "print(\"Lasso Accuracy=\",L_accuracy)\n",
    "print(\"X-axis Accuracy=\",X_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y_true and y_pred have different number of output (1000!=1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-e1200c3318cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_testB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXaxis_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_testB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \"\"\"\n\u001b[0;32m    240\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 241\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         raise ValueError(\"y_true and y_pred have different number of output \"\n\u001b[1;32m---> 89\u001b[1;33m                          \"({0}!={1})\".format(y_true.shape[1], y_pred.shape[1]))\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mn_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: y_true and y_pred have different number of output (1000!=1)"
     ]
    }
   ],
   "source": [
    "print(len(y_testB))\n",
    "print(mean_squared_error(Xaxis_pred, y_testB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
